{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best-Score Pipeline (리더 보드 0.775513375)\n",
        "- LGBM: feature selection only\n",
        "- Final: Deep Learning (MLP)\n",
        "- Submit: P(voted==2) with sample_submission format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요할 때만 실행\n",
        "# !pip install --default-timeout=300 lightgbm tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/test: (45532, 78) (11383, 77) pos_ratio: 0.5468242115435298\n",
            "groups: QA 20 QE 20 TP 10 WR 13 WF 3\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \"requirements.txt\").exists() or (p / \"README.md\").exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(\"프로젝트 루트를 찾지 못했습니다. vote-AI 루트에 requirements.txt 또는 README.md가 있는지 확인하세요.\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "\n",
        "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "test  = pd.read_csv(DATA_DIR / \"test_x.csv\")\n",
        "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
        "\n",
        "y = (train[\"voted\"] == 2).astype(\"int32\")\n",
        "X_raw = train.drop(columns=[\"voted\"])\n",
        "\n",
        "cols = list(X_raw.columns)\n",
        "q_like = [c for c in cols if re.match(r\"^Q\", c)]\n",
        "QA_cols = sorted([c for c in q_like if re.fullmatch(r\"Q[a-z]A\", c)])\n",
        "QE_cols = sorted([c for c in q_like if re.fullmatch(r\"Q[a-z]E\", c)])\n",
        "TP_cols = sorted([c for c in cols if re.fullmatch(r\"tp\\d{2}\", c)])\n",
        "WR_cols = sorted([c for c in cols if re.fullmatch(r\"wr_?\\d{2}\", c)])\n",
        "WF_cols = sorted([c for c in cols if re.fullmatch(r\"wf_?\\d{2}\", c)])\n",
        "\n",
        "print(\"train/test:\", train.shape, test.shape, \"pos_ratio:\", float(y.mean()))\n",
        "print(\"groups:\", \"QA\", len(QA_cols), \"QE\", len(QE_cols), \"TP\", len(TP_cols), \"WR\", len(WR_cols), \"WF\", len(WF_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_feat/T_feat: (45532, 43) (11383, 43)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/wb/_9w5mhfj4kl4vxshnbz2n7p80000gn/T/ipykernel_46554/339422939.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
            "/var/folders/wb/_9w5mhfj4kl4vxshnbz2n7p80000gn/T/ipykernel_46554/339422939.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n"
          ]
        }
      ],
      "source": [
        "USE_REVERSE_MACH = True\n",
        "\n",
        "FLIP_PUBLIC = [\"QeA\",\"QfA\",\"QkA\",\"QqA\",\"QrA\"]\n",
        "FLIP_SECRET = [\"QaA\",\"QdA\",\"QgA\",\"QiA\",\"QnA\"]\n",
        "\n",
        "def age_group_to_ord(v):\n",
        "    try: return int(str(v).replace(\"s\",\"\"))\n",
        "    except: return -1\n",
        "\n",
        "def urban_to_ord(v):\n",
        "    try:\n",
        "        iv=int(v); return -1 if iv==0 else iv\n",
        "    except: return -1\n",
        "\n",
        "def fe(df: pd.DataFrame, tp_means=None):\n",
        "    df = df.copy()\n",
        "    if \"index\" in df.columns:\n",
        "        df = df.drop(columns=[\"index\"])\n",
        "\n",
        "    for c in QA_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in QE_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in TP_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in WR_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in WF_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    if \"age_group\" in df.columns:\n",
        "        df[\"age_group_ord\"] = df[\"age_group\"].astype(str).apply(age_group_to_ord)\n",
        "    if \"urban\" in df.columns:\n",
        "        df[\"urban_ord\"] = df[\"urban\"].apply(urban_to_ord)\n",
        "\n",
        "    for c in [\"education\",\"urban\",\"hand\",\"married\"]:\n",
        "        if c in df.columns:\n",
        "            tmp = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "            df[f\"{c}_is_missing\"] = (tmp==0).astype(\"int32\")\n",
        "\n",
        "    if USE_REVERSE_MACH:\n",
        "        for c in FLIP_PUBLIC + FLIP_SECRET:\n",
        "            if c in df.columns: df[c] = 6 - df[c]\n",
        "\n",
        "    qa = df[QA_cols]\n",
        "    df[\"qa_mean\"] = qa.mean(axis=1)\n",
        "    df[\"qa_std\"] = qa.std(axis=1)\n",
        "    df[\"neutral_ratio\"] = (qa==3).mean(axis=1)\n",
        "    df[\"confident_ratio\"] = ((qa==1)|(qa==5)).mean(axis=1)\n",
        "    df[\"qa_missing_ratio\"] = qa.isna().mean(axis=1)\n",
        "\n",
        "    if USE_REVERSE_MACH:\n",
        "        df[\"T\"] = df[\"QcA\"] - df[\"QfA\"] + df[\"QoA\"] - df[\"QrA\"] + df[\"QsA\"]\n",
        "        df[\"V\"] = df[\"QbA\"] - df[\"QeA\"] + df[\"QhA\"] + df[\"QjA\"] + df[\"QmA\"] - df[\"QqA\"]\n",
        "        df[\"M\"] = -df[\"QkA\"]\n",
        "        df[\"Mach_score\"] = qa.mean(axis=1)\n",
        "\n",
        "    qe = df[QE_cols].clip(lower=0)\n",
        "    qe_log = np.log1p(qe)\n",
        "    df[\"delay_sum\"] = qe.sum(axis=1)\n",
        "    df[\"delay_log\"] = np.log1p(df[\"delay_sum\"])\n",
        "    df[\"delay_std\"] = qe.std(axis=1)\n",
        "    df[\"qe_fast_ratio\"] = (qe_log<1.0).mean(axis=1)\n",
        "    df[\"qe_slow_ratio\"] = (qe_log>4.0).mean(axis=1)\n",
        "\n",
        "    for c in TP_cols:\n",
        "        df.loc[df[c]==0, c] = np.nan\n",
        "    if tp_means is None:\n",
        "        tp_means = {c: float(df[c].mean()) for c in TP_cols}\n",
        "    for c in TP_cols:\n",
        "        df[c] = df[c].fillna(tp_means[c])\n",
        "\n",
        "    df[\"Ex_diff\"] = df[\"tp01\"]-df[\"tp06\"]; df[\"Ex_strength\"]=df[\"Ex_diff\"].abs()\n",
        "    df[\"Ag_diff\"] = df[\"tp07\"]-df[\"tp02\"]; df[\"Ag_strength\"]=df[\"Ag_diff\"].abs()\n",
        "    df[\"Con_diff\"]= df[\"tp03\"]-df[\"tp08\"]; df[\"Con_strength\"]=df[\"Con_diff\"].abs()\n",
        "    df[\"Es_diff\"] = df[\"tp09\"]-df[\"tp04\"]; df[\"Es_strength\"]=df[\"Es_diff\"].abs()\n",
        "    df[\"Op_diff\"] = df[\"tp05\"]-df[\"tp10\"]; df[\"Op_strength\"]=df[\"Op_diff\"].abs()\n",
        "\n",
        "    if len(WR_cols)>0: df[\"wr_sum\"] = df[WR_cols].sum(axis=1)\n",
        "    else: df[\"wr_sum\"]=0\n",
        "    if len(WF_cols)>0: df[\"wf_sum\"] = df[WF_cols].sum(axis=1)\n",
        "    else: df[\"wf_sum\"]=0\n",
        "    df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
        "\n",
        "    # drop raw groups (keep summaries only)\n",
        "    df = df.drop(columns=[c for c in QA_cols+QE_cols+TP_cols+WR_cols+WF_cols if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "    # simple encoding for remaining categorical strings\n",
        "    for c in [\"gender\",\"race\",\"religion\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype(\"category\").cat.codes.astype(\"int32\")\n",
        "\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
        "    return df, tp_means\n",
        "\n",
        "X_feat, tp_means_all = fe(X_raw.copy(), tp_means=None)\n",
        "T_feat, _ = fe(test.copy(), tp_means=tp_means_all)\n",
        "\n",
        "print(\"X_feat/T_feat:\", X_feat.shape, T_feat.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019319 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1648\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 400 rounds\n",
            "Early stopping, best iteration is:\n",
            "[164]\tvalid_0's auc: 0.779079\tvalid_0's binary_logloss: 0.550402\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1649\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 400 rounds\n",
            "Early stopping, best iteration is:\n",
            "[202]\tvalid_0's auc: 0.769152\tvalid_0's binary_logloss: 0.555657\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16508\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008315 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1647\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546807 -> initscore=0.187779\n",
            "[LightGBM] [Info] Start training from score 0.187779\n",
            "Training until validation scores don't improve for 400 rounds\n",
            "Early stopping, best iteration is:\n",
            "[210]\tvalid_0's auc: 0.763346\tvalid_0's binary_logloss: 0.561604\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002823 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1647\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 400 rounds\n",
            "Early stopping, best iteration is:\n",
            "[194]\tvalid_0's auc: 0.761018\tvalid_0's binary_logloss: 0.566159\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002610 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1648\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 400 rounds\n",
            "Early stopping, best iteration is:\n",
            "[214]\tvalid_0's auc: 0.767983\tvalid_0's binary_logloss: 0.558608\n",
            "LGBM mean AUC: 0.7681157258081542 std: 0.00623571696773741\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004964 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1527\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1526\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 30 quick AUC 0.7540681569620524\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1643\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 40 quick AUC 0.7562075397565312\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003408 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1643\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 60 quick AUC 0.7562075397565312\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1643\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002305 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 80 quick AUC 0.7562075397565312\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003175 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1643\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 120 quick AUC 0.7562075397565312\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002395 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006231 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1643\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002962 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1644\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "K 160 quick AUC 0.7562075397565312\n",
            "Selected TOPK: 40\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "\n",
        "X_lgb = X_feat.copy()\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "importances = np.zeros(X_lgb.shape[1], dtype=float)\n",
        "auc_list = []\n",
        "\n",
        "for fold,(tr,va) in enumerate(skf.split(X_lgb, y)):\n",
        "    X_tr, X_va = X_lgb.iloc[tr], X_lgb.iloc[va]\n",
        "    y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
        "\n",
        "    m = LGBMClassifier(n_estimators=8000, learning_rate=0.015, num_leaves=255,\n",
        "                       subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "                       random_state=42+fold)\n",
        "    m.fit(X_tr, y_tr, eval_set=[(X_va,y_va)], eval_metric=\"auc\",\n",
        "          callbacks=[early_stopping(400), log_evaluation(0)])\n",
        "    p = m.predict_proba(X_va)[:,1]\n",
        "    auc_list.append(roc_auc_score(y_va, p))\n",
        "    importances += m.booster_.feature_importance(importance_type=\"gain\")\n",
        "\n",
        "print(\"LGBM mean AUC:\", float(np.mean(auc_list)), \"std:\", float(np.std(auc_list)))\n",
        "\n",
        "imp_rank = pd.Series(importances, index=X_lgb.columns).sort_values(ascending=False)\n",
        "\n",
        "K_CANDIDATES = [30,40,60,80,120,160]\n",
        "best_k,best_auc=None,-1\n",
        "skf2 = StratifiedKFold(n_splits=3, shuffle=True, random_state=777)\n",
        "for k in K_CANDIDATES:\n",
        "    use = imp_rank.head(k).index.tolist()\n",
        "    auc2=[]\n",
        "    for f,(tr2,va2) in enumerate(skf2.split(X_lgb[use], y)):\n",
        "        mm = LGBMClassifier(n_estimators=3000, learning_rate=0.02, num_leaves=127, random_state=1000+f)\n",
        "        mm.fit(X_lgb[use].iloc[tr2], y.iloc[tr2])\n",
        "        pp = mm.predict_proba(X_lgb[use].iloc[va2])[:,1]\n",
        "        auc2.append(roc_auc_score(y.iloc[va2], pp))\n",
        "    mean_auc2=float(np.mean(auc2))\n",
        "    print(\"K\",k,\"quick AUC\",mean_auc2)\n",
        "    if mean_auc2>best_auc:\n",
        "        best_auc, best_k = mean_auc2, k\n",
        "\n",
        "TOPK = best_k\n",
        "selected_features = imp_rank.head(TOPK).index.tolist()\n",
        "print(\"Selected TOPK:\", TOPK)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODE: FAST CFG: {'EPOCHS': 12, 'N_SPLITS': 3, 'SEEDS': [42], 'BATCH': 512}\n",
            "[seed 42] fold 0 AUC=0.764733\n",
            "[seed 42] fold 1 AUC=0.756909\n",
            "[seed 42] fold 2 AUC=0.755138\n",
            "\n",
            "[seed 42] OOF AUC: 0.758823\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ Ensemble OOF AUC: 0.7588231633801006\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X_sel = X_feat[selected_features].copy()\n",
        "T_sel = T_feat[selected_features].copy()\n",
        "\n",
        "def make_ds(Xdf, yarr=None, batch_size=512, shuffle=False, seed=42):\n",
        "    Xnp = Xdf.values.astype(\"float32\")\n",
        "    if yarr is None:\n",
        "        ds = tf.data.Dataset.from_tensor_slices(Xnp)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_tensor_slices((Xnp, yarr.astype(\"float32\")))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=min(len(Xdf), 10000), seed=seed, reshuffle_each_iteration=True)\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def build_mlp(input_dim, lr=1e-3, dropout=0.3):\n",
        "    inp = keras.Input(shape=(input_dim,), dtype=tf.float32)\n",
        "    x = layers.BatchNormalization()(inp)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[keras.metrics.AUC(name=\"auc\")])\n",
        "    return model\n",
        "\n",
        "FAST = dict(EPOCHS=12, N_SPLITS=3, SEEDS=[42], BATCH=512)\n",
        "SUBMIT = dict(EPOCHS=25, N_SPLITS=5, SEEDS=[42,202,777], BATCH=256)\n",
        "MODE=\"FAST\"\n",
        "CFG = FAST if MODE==\"FAST\" else SUBMIT\n",
        "print(\"MODE:\", MODE, \"CFG:\", CFG)\n",
        "\n",
        "EPOCHS=CFG[\"EPOCHS\"]; N_SPLITS=CFG[\"N_SPLITS\"]; SEEDS=CFG[\"SEEDS\"]; BATCH=CFG[\"BATCH\"]\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "oof_by_seed=[]\n",
        "for seed in SEEDS:\n",
        "    oof=np.zeros(len(X_sel), dtype=\"float32\")\n",
        "    for fold,(tr,va) in enumerate(skf.split(X_sel, y)):\n",
        "        X_tr=X_sel.iloc[tr].reset_index(drop=True)\n",
        "        y_tr=y.iloc[tr].reset_index(drop=True).values\n",
        "        X_va=X_sel.iloc[va].reset_index(drop=True)\n",
        "        y_va=y.iloc[va].reset_index(drop=True).values\n",
        "\n",
        "        pos=float(y_tr.mean()); neg=1.0-pos\n",
        "        class_weight={0:1.0, 1:neg/(pos+1e-9)}\n",
        "\n",
        "        tf.keras.utils.set_random_seed(seed+fold)\n",
        "        model = build_mlp(X_tr.shape[1], lr=1e-3, dropout=0.3)\n",
        "\n",
        "        tr_ds=make_ds(X_tr, y_tr, batch_size=BATCH, shuffle=True, seed=seed+fold)\n",
        "        va_ds=make_ds(X_va, y_va, batch_size=BATCH, shuffle=False)\n",
        "\n",
        "        cb=[\n",
        "            keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=1, min_lr=1e-5),\n",
        "        ]\n",
        "        model.fit(tr_ds, validation_data=va_ds, epochs=EPOCHS, verbose=0, callbacks=cb, class_weight=class_weight)\n",
        "\n",
        "        pred=model.predict(make_ds(X_va, None, batch_size=BATCH), verbose=0).reshape(-1)\n",
        "        pred=np.nan_to_num(pred, nan=0.5)\n",
        "        oof[va]=pred\n",
        "        print(f\"[seed {seed}] fold {fold} AUC={roc_auc_score(y_va, pred):.6f}\")\n",
        "\n",
        "    auc_seed=roc_auc_score(y, oof)\n",
        "    oof_by_seed.append(oof)\n",
        "    print(f\"\\n[seed {seed}] OOF AUC: {auc_seed:.6f}\")\n",
        "    print('-'*60)\n",
        "\n",
        "oof_ens=np.mean(np.vstack(oof_by_seed), axis=0)\n",
        "print(\"\\n✅ Ensemble OOF AUC:\", roc_auc_score(y, oof_ens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission: (11383, 2) ['index', 'voted']\n",
            "count    11383.000000\n",
            "mean         0.520519\n",
            "std          0.244247\n",
            "min          0.087032\n",
            "25%          0.331329\n",
            "50%          0.432507\n",
            "75%          0.716390\n",
            "max          0.998735\n",
            "Name: voted, dtype: float64\n",
            "Saved: /Users/admin/Downloads/AI 헬스케어 수업/oz코딩 수업/해커톤 (1)/vote-AI/submission_best.csv\n"
          ]
        }
      ],
      "source": [
        "# ---- Final submission (딥러닝-only) ----\n",
        "FINAL_SEEDS = SUBMIT[\"SEEDS\"]\n",
        "EPOCHS_SUB = SUBMIT[\"EPOCHS\"]\n",
        "BATCH_SUB = SUBMIT[\"BATCH\"]\n",
        "\n",
        "def train_full_predict(seed):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    model = build_mlp(X_sel.shape[1], lr=1e-3, dropout=0.3)\n",
        "\n",
        "    pos=float(y.mean()); neg=1.0-pos\n",
        "    class_weight={0:1.0, 1:neg/(pos+1e-9)}\n",
        "\n",
        "    tr_ds=make_ds(X_sel, y.values, batch_size=BATCH_SUB, shuffle=True, seed=seed)\n",
        "    model.fit(tr_ds, epochs=max(10, EPOCHS_SUB//2), verbose=0, class_weight=class_weight)\n",
        "    pred=model.predict(make_ds(T_sel, None, batch_size=BATCH_SUB), verbose=0).reshape(-1)\n",
        "    return np.nan_to_num(pred, nan=0.5)\n",
        "\n",
        "pred_test = np.mean(np.vstack([train_full_predict(s) for s in FINAL_SEEDS]), axis=0).reshape(-1)\n",
        "\n",
        "submission = sub.copy()\n",
        "submission[\"voted\"] = pred_test\n",
        "\n",
        "print(\"submission:\", submission.shape, submission.columns.tolist())\n",
        "print(submission[\"voted\"].describe())\n",
        "\n",
        "assert submission.shape == (11383, 2)\n",
        "assert submission.columns.tolist() == [\"index\",\"voted\"]\n",
        "assert float(submission[\"voted\"].min()) >= 0.0 and float(submission[\"voted\"].max()) <= 1.0\n",
        "\n",
        "out_path = PROJECT_ROOT / \"submission_best.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9ebb33",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "voteai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
