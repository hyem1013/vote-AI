{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Score (Final DL) — LGBM Feature Selection + Embedding MLP (Auto-Choose)\n",
        "\n",
        "목표: 가능한 한 높은 점수 (규칙 준수)\n",
        "\n",
        "규칙 준수\n",
        "- 최종 제출 모델: 딥러닝(Embedding + MLP)\n",
        "- LightGBM: 피처 중요도/선별용 도구로만 사용\n",
        "\n",
        "이 노트북이 자동으로 하는 것\n",
        "1) Feature Engineering 2가지 옵션(A: reverse+mach, B: no reverse+mach)\n",
        "2) 옵션별로 LightGBM 중요도 기반 Top-K numeric 선택\n",
        "3) (빠른 검증) 3-fold/seed=42로 옵션/TopK 조합을 자동 선택\n",
        "4) (제출) 선택된 설정으로 5-fold + seed 앙상블(42/202/777)로 학습 후 제출 파일 생성\n",
        "\n",
        "제출\n",
        "- voted 컬럼에는 P(voted==2)만 저장\n",
        "- sample_submission.csv 형식(index+voted) 강제\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요할 때만 실행하세요.\n",
        "# !pip install --default-timeout=300 lightgbm tensorflow scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 데이터 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/test/sub: (45532, 78) (11383, 77) (11383, 2)\n",
            "pos_ratio(voted==2): 0.5468242115435298\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "def find_project_root(start: Path) -> Path:\n",
        "    start = start.resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / \"requirements.txt\").exists() or (p / \"README.md\").exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(\"프로젝트 루트를 찾지 못했습니다. vote-AI 루트에 requirements.txt 또는 README.md가 있는지 확인하세요.\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root(Path.cwd())\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "\n",
        "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "test  = pd.read_csv(DATA_DIR / \"test_x.csv\")\n",
        "sub   = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
        "\n",
        "# ✅ target fixed: voted==2 is positive\n",
        "y = (train[\"voted\"] == 2).astype(\"int32\")\n",
        "X_raw = train.drop(columns=[\"voted\"])\n",
        "\n",
        "print(\"train/test/sub:\", train.shape, test.shape, sub.shape)\n",
        "print(\"pos_ratio(voted==2):\", float(y.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 컬럼 그룹 탐지\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA/QE/TP/WR/WF: 20 20 10 13 3\n"
          ]
        }
      ],
      "source": [
        "cols = list(X_raw.columns)\n",
        "q_like = [c for c in cols if c.startswith(\"Q\")]\n",
        "\n",
        "QA_cols = sorted([c for c in q_like if re.fullmatch(r\"Q[a-z]A\", c)])\n",
        "QE_cols = sorted([c for c in q_like if re.fullmatch(r\"Q[a-z]E\", c)])\n",
        "\n",
        "TP_cols = sorted([c for c in cols if re.fullmatch(r\"tp\\d{2}\", c)])\n",
        "WR_cols = sorted([c for c in cols if re.fullmatch(r\"wr_?\\d{2}\", c)])\n",
        "WF_cols = sorted([c for c in cols if re.fullmatch(r\"wf_?\\d{2}\", c)])\n",
        "\n",
        "print(\"QA/QE/TP/WR/WF:\", len(QA_cols), len(QE_cols), len(TP_cols), len(WR_cols), len(WF_cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Feature Engineering (옵션 A/B)\n",
        "- 공통: 태도(QA) 요약 + 응답시간(QE) 요약 + Big5(diff/strength) + 단어(wr/wf) 요약 + 결측 indicator\n",
        "- 옵션 A: reverse+mach 포함\n",
        "- 옵션 B: reverse+mach 제외\n",
        "\n",
        "주의: 학습/선별에 필요한 정보는 남기되, 노이즈 큰 원본 그룹(QE, TP, WR/WF)은 요약 후 제거합니다.\n",
        "QA 원본은 (옵션 A에서 reverse 반영 후) LGBM이 중요 문항을 고를 수 있게 'numeric 후보군'으로 남깁니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OptionA numeric dim: (45532, 54) OptionB numeric dim: (45532, 50)\n"
          ]
        }
      ],
      "source": [
        "FLIP_PUBLIC = [\"QeA\",\"QfA\",\"QkA\",\"QqA\",\"QrA\"]\n",
        "FLIP_SECRET = [\"QaA\",\"QdA\",\"QgA\",\"QiA\",\"QnA\"]\n",
        "TPS = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
        "\n",
        "CAT_COLS = [\"age_group\",\"gender\",\"race\",\"religion\",\"education\",\"engnat\",\"married\",\"urban\",\"hand\"]\n",
        "MISS0_COLS = [\"education\",\"urban\",\"hand\",\"married\"]\n",
        "\n",
        "def age_group_to_ord(v):\n",
        "    try: return int(str(v).replace(\"s\",\"\"))\n",
        "    except: return -1\n",
        "\n",
        "def urban_to_ord(v):\n",
        "    try:\n",
        "        iv = int(v)\n",
        "        return -1 if iv == 0 else iv\n",
        "    except:\n",
        "        return -1\n",
        "\n",
        "def build_features(df: pd.DataFrame, *, tp_means=None, use_reverse_mach=False):\n",
        "    df = df.copy()\n",
        "\n",
        "    # keep index out of features\n",
        "    if \"index\" in df.columns:\n",
        "        df = df.drop(columns=[\"index\"])\n",
        "\n",
        "    # numeric cast\n",
        "    for c in QA_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in QE_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in TP_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in WR_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    for c in WF_cols: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # missing indicators from 0\n",
        "    for c in MISS0_COLS:\n",
        "        if c in df.columns:\n",
        "            tmp = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "            df[f\"{c}_is_missing\"] = (tmp == 0).astype(\"int32\")\n",
        "\n",
        "    # ord features\n",
        "    if \"age_group\" in df.columns:\n",
        "        df[\"age_group_ord\"] = df[\"age_group\"].astype(str).apply(age_group_to_ord)\n",
        "    if \"urban\" in df.columns:\n",
        "        df[\"urban_ord\"] = df[\"urban\"].apply(urban_to_ord)\n",
        "\n",
        "    # QA reverse (optional)\n",
        "    if use_reverse_mach:\n",
        "        for c in FLIP_PUBLIC + FLIP_SECRET:\n",
        "            if c in df.columns:\n",
        "                df[c] = 6 - df[c]\n",
        "\n",
        "    # QA style\n",
        "    qa = df[QA_cols]\n",
        "    df[\"qa_mean\"] = qa.mean(axis=1)\n",
        "    df[\"qa_std\"]  = qa.std(axis=1)\n",
        "    df[\"neutral_ratio\"] = (qa == 3).mean(axis=1)\n",
        "    df[\"extreme_ratio\"] = ((qa == 1) | (qa == 5)).mean(axis=1)\n",
        "    df[\"qa_missing_ratio\"] = qa.isna().mean(axis=1)\n",
        "\n",
        "    # Mach (optional)\n",
        "    if use_reverse_mach:\n",
        "        df[\"mach_T\"] = df[\"QcA\"] - df[\"QfA\"] + df[\"QoA\"] - df[\"QrA\"] + df[\"QsA\"]\n",
        "        df[\"mach_V\"] = df[\"QbA\"] - df[\"QeA\"] + df[\"QhA\"] + df[\"QjA\"] + df[\"QmA\"] - df[\"QqA\"]\n",
        "        df[\"mach_M\"] = -df[\"QkA\"]\n",
        "        df[\"mach_mean\"] = qa.mean(axis=1)\n",
        "\n",
        "    # QE delay (drop QE after)\n",
        "    qe = df[QE_cols].clip(lower=0)\n",
        "    qe_log = np.log1p(qe)\n",
        "    df[\"delay_sum\"] = qe.sum(axis=1)\n",
        "    df[\"delay_log\"] = np.log1p(df[\"delay_sum\"])\n",
        "    df[\"delay_std\"] = qe.std(axis=1)\n",
        "    df[\"qe_fast_ratio\"] = (qe_log < 1.0).mean(axis=1)\n",
        "    df[\"qe_slow_ratio\"] = (qe_log > 4.0).mean(axis=1)\n",
        "    df = df.drop(columns=QE_cols, errors=\"ignore\")\n",
        "\n",
        "    # WR/WF summary (drop raw after)\n",
        "    df[\"wr_sum\"] = df[WR_cols].sum(axis=1) if len(WR_cols) else 0\n",
        "    df[\"wf_sum\"] = df[WF_cols].sum(axis=1) if len(WF_cols) else 0\n",
        "    df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
        "    df = df.drop(columns=WR_cols + WF_cols, errors=\"ignore\")\n",
        "\n",
        "    # TIPI -> big5 diff/strength (drop TP raw after)\n",
        "    for c in TP_cols:\n",
        "        df.loc[df[c] == 0, c] = np.nan\n",
        "    if tp_means is None:\n",
        "        tp_means = {c: float(df[c].mean()) for c in TP_cols}\n",
        "    for c in TP_cols:\n",
        "        df[c] = df[c].fillna(tp_means[c])\n",
        "\n",
        "    df[\"Ex_diff\"] = df[\"tp01\"]-df[\"tp06\"]; df[\"Ex_strength\"]=df[\"Ex_diff\"].abs()\n",
        "    df[\"Ag_diff\"] = df[\"tp07\"]-df[\"tp02\"]; df[\"Ag_strength\"]=df[\"Ag_diff\"].abs()\n",
        "    df[\"Con_diff\"]= df[\"tp03\"]-df[\"tp08\"]; df[\"Con_strength\"]=df[\"Con_diff\"].abs()\n",
        "    df[\"Es_diff\"] = df[\"tp09\"]-df[\"tp04\"]; df[\"Es_strength\"]=df[\"Es_diff\"].abs()\n",
        "    df[\"Op_diff\"] = df[\"tp05\"]-df[\"tp10\"]; df[\"Op_strength\"]=df[\"Op_diff\"].abs()\n",
        "    df = df.drop(columns=TP_cols, errors=\"ignore\")\n",
        "\n",
        "    # categorical columns as string (for embedding)\n",
        "    cat_df = df[CAT_COLS].copy()\n",
        "    for c in CAT_COLS:\n",
        "        cat_df[c] = cat_df[c].astype(str)\n",
        "\n",
        "    # numeric columns: everything else\n",
        "    num_df = df.drop(columns=CAT_COLS, errors=\"ignore\").apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
        "\n",
        "    return cat_df, num_df, tp_means\n",
        "\n",
        "# Build option A/B\n",
        "catA, numA, tp_means = build_features(X_raw, tp_means=None, use_reverse_mach=True)\n",
        "catB, numB, _ = build_features(X_raw, tp_means=tp_means, use_reverse_mach=False)  # tp_means fixed from train\n",
        "\n",
        "tcatA, tnumA, _ = build_features(test, tp_means=tp_means, use_reverse_mach=True)\n",
        "tcatB, tnumB, _ = build_features(test, tp_means=tp_means, use_reverse_mach=False)\n",
        "\n",
        "print(\"OptionA numeric dim:\", numA.shape, \"OptionB numeric dim:\", numB.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) LGBM으로 numeric-only 중요도 → TopK 선택 (옵션별)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001866 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1727\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[170]\tvalid_0's auc: 0.745379\tvalid_0's binary_logloss: 0.583377\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001822 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1728\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[212]\tvalid_0's auc: 0.737204\tvalid_0's binary_logloss: 0.590654\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16508\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001975 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1726\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546807 -> initscore=0.187779\n",
            "[LightGBM] [Info] Start training from score 0.187779\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[190]\tvalid_0's auc: 0.734564\tvalid_0's binary_logloss: 0.590927\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002322 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1726\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[229]\tvalid_0's auc: 0.73041\tvalid_0's binary_logloss: 0.596292\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1727\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[235]\tvalid_0's auc: 0.733653\tvalid_0's binary_logloss: 0.594542\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1585\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1583\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1584\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1722\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001486 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1722\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001866 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1722\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001299 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1722\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1723\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 52\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001886 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1562\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[259]\tvalid_0's auc: 0.745847\tvalid_0's binary_logloss: 0.582003\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1562\n",
            "[LightGBM] [Info] Number of data points in the train set: 36425, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546822 -> initscore=0.187839\n",
            "[LightGBM] [Info] Start training from score 0.187839\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[267]\tvalid_0's auc: 0.738439\tvalid_0's binary_logloss: 0.589564\n",
            "[LightGBM] [Info] Number of positive: 19918, number of negative: 16508\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001816 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1562\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546807 -> initscore=0.187779\n",
            "[LightGBM] [Info] Start training from score 0.187779\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[144]\tvalid_0's auc: 0.733498\tvalid_0's binary_logloss: 0.592369\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001816 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1562\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[183]\tvalid_0's auc: 0.730482\tvalid_0's binary_logloss: 0.596359\n",
            "[LightGBM] [Info] Number of positive: 19919, number of negative: 16507\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1564\n",
            "[LightGBM] [Info] Number of data points in the train set: 36426, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546835 -> initscore=0.187890\n",
            "[LightGBM] [Info] Start training from score 0.187890\n",
            "Training until validation scores don't improve for 250 rounds\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's auc: 0.733471\tvalid_0's binary_logloss: 0.595037\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1519\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1515\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1516\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001690 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1559\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1555\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1556\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1559\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1555\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1556\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001494 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1559\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001638 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1555\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1556\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16598, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1559\n",
            "[LightGBM] [Info] Number of data points in the train set: 30354, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546814 -> initscore=0.187807\n",
            "[LightGBM] [Info] Start training from score 0.187807\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1555\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "[LightGBM] [Info] Number of positive: 16599, number of negative: 13756\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001387 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1556\n",
            "[LightGBM] [Info] Number of data points in the train set: 30355, number of used features: 48\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.546829 -> initscore=0.187867\n",
            "[LightGBM] [Info] Start training from score 0.187867\n",
            "LGB A mean AUC: 0.7362420003704049 TopK: 40 selected: 40\n",
            "LGB B mean AUC: 0.736347664036931 TopK: 40 selected: 40\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def lgb_select(num_df: pd.DataFrame, y: pd.Series, k_candidates=(40,60,80,120,160)):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    importances = np.zeros(num_df.shape[1], dtype=float)\n",
        "    aucs = []\n",
        "    for fold,(tr,va) in enumerate(skf.split(num_df, y)):\n",
        "        X_tr, X_va = num_df.iloc[tr], num_df.iloc[va]\n",
        "        y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
        "        m = LGBMClassifier(\n",
        "            n_estimators=6000, learning_rate=0.02, num_leaves=127,\n",
        "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "            random_state=42+fold\n",
        "        )\n",
        "        m.fit(X_tr, y_tr, eval_set=[(X_va,y_va)], eval_metric=\"auc\",\n",
        "              callbacks=[early_stopping(250), log_evaluation(0)])\n",
        "        p = m.predict_proba(X_va)[:,1]\n",
        "        aucs.append(roc_auc_score(y_va, p))\n",
        "        importances += m.booster_.feature_importance(importance_type=\"gain\")\n",
        "    imp = pd.Series(importances, index=num_df.columns).sort_values(ascending=False)\n",
        "\n",
        "    # quick choose K\n",
        "    skf2 = StratifiedKFold(n_splits=3, shuffle=True, random_state=777)\n",
        "    best_k, best_auc = None, -1\n",
        "    for k in k_candidates:\n",
        "        use = imp.head(k).index.tolist()\n",
        "        a2=[]\n",
        "        for f,(tr,va) in enumerate(skf2.split(num_df[use], y)):\n",
        "            mm = LGBMClassifier(n_estimators=2500, learning_rate=0.03, random_state=1000+f)\n",
        "            mm.fit(num_df[use].iloc[tr], y.iloc[tr])\n",
        "            pp = mm.predict_proba(num_df[use].iloc[va])[:,1]\n",
        "            a2.append(roc_auc_score(y.iloc[va], pp))\n",
        "        mean_a2 = float(np.mean(a2))\n",
        "        if mean_a2 > best_auc:\n",
        "            best_auc, best_k = mean_a2, k\n",
        "    return imp, best_k, float(np.mean(aucs))\n",
        "\n",
        "impA, topkA, lgbA = lgb_select(numA, y, k_candidates=(40,60,80,120,160))\n",
        "impB, topkB, lgbB = lgb_select(numB, y, k_candidates=(40,60,80,120,160))\n",
        "\n",
        "selA = impA.head(topkA).index.tolist()\n",
        "selB = impB.head(topkB).index.tolist()\n",
        "\n",
        "print(\"LGB A mean AUC:\", lgbA, \"TopK:\", topkA, \"selected:\", len(selA))\n",
        "print(\"LGB B mean AUC:\", lgbB, \"TopK:\", topkB, \"selected:\", len(selB))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) 딥러닝 모델 (Embedding + MLP) — 옵션 A/B quick OOF 비교 후 자동 선택\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DL quick AUC A: 0.7566092269622803 DL quick AUC B: 0.7565507923680754\n",
            "✅ Selected option: A(reverse+mach)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def df_to_ds(cat_df, num_df, y_=None, batch=512, shuffle=False, seed=202):\n",
        "    feats = {c: cat_df[c].astype(str).values for c in cat_df.columns}\n",
        "    feats[\"num\"] = num_df.values.astype(\"float32\")\n",
        "    if y_ is None:\n",
        "        ds = tf.data.Dataset.from_tensor_slices(feats)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_tensor_slices((feats, y_.values.astype(\"float32\")))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=min(len(num_df), 10000), seed=seed, reshuffle_each_iteration=True)\n",
        "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def build_embed_mlp(cat_df_train, num_train, emb_dim=16, lr=3e-4, dropout=0.3):\n",
        "    inputs = {}\n",
        "    enc = []\n",
        "\n",
        "    # categorical embeddings\n",
        "    for c in cat_df_train.columns:\n",
        "        inp = keras.Input(shape=(1,), name=c, dtype=tf.string)\n",
        "        lookup = layers.StringLookup(output_mode=\"int\")\n",
        "        lookup.adapt(cat_df_train[c].astype(str).values)  # train fold only\n",
        "        vocab = lookup.vocabulary_size()\n",
        "        dim = min(emb_dim, max(2, int(np.ceil(vocab**0.25)*2)))\n",
        "        x = lookup(inp)\n",
        "        x = layers.Embedding(vocab, dim)(x)\n",
        "        x = layers.Reshape((dim,))(x)\n",
        "        inputs[c]=inp\n",
        "        enc.append(x)\n",
        "\n",
        "    # numeric\n",
        "    num_inp = keras.Input(shape=(num_train.shape[1],), name=\"num\", dtype=tf.float32)\n",
        "    norm = layers.Normalization()\n",
        "    norm.adapt(num_train.values.astype(\"float32\"))  # train fold only\n",
        "    xnum = norm(num_inp)\n",
        "    inputs[\"num\"]=num_inp\n",
        "    enc.append(xnum)\n",
        "\n",
        "    x = layers.Concatenate()(enc)\n",
        "    x = layers.Dense(256)(x); x = layers.BatchNormalization()(x); x = layers.Activation(\"relu\")(x); x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(128)(x); x = layers.BatchNormalization()(x); x = layers.Activation(\"relu\")(x); x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer=keras.optimizers.AdamW(learning_rate=lr, weight_decay=3e-4),\n",
        "                  loss=\"binary_crossentropy\", metrics=[keras.metrics.AUC(name=\"auc\")])\n",
        "    return model\n",
        "\n",
        "def quick_oof(cat_df, num_df, y, epochs=12, splits=3, seed=42, batch=512):\n",
        "    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(num_df), dtype=\"float32\")\n",
        "    for fold,(tr,va) in enumerate(skf.split(num_df, y)):\n",
        "        cat_tr, cat_va = cat_df.iloc[tr].reset_index(drop=True), cat_df.iloc[va].reset_index(drop=True)\n",
        "        num_tr, num_va = num_df.iloc[tr].reset_index(drop=True), num_df.iloc[va].reset_index(drop=True)\n",
        "        y_tr, y_va = y.iloc[tr].reset_index(drop=True), y.iloc[va].reset_index(drop=True)\n",
        "\n",
        "        pos=float(y_tr.mean()); neg=1.0-pos\n",
        "        class_weight={0:1.0, 1:neg/(pos+1e-9)}\n",
        "\n",
        "        tf.keras.utils.set_random_seed(seed+fold)\n",
        "        model = build_embed_mlp(cat_tr, num_tr)\n",
        "\n",
        "        tr_ds = df_to_ds(cat_tr, num_tr, y_tr, batch=batch, shuffle=True, seed=seed+fold)\n",
        "        va_ds = df_to_ds(cat_va, num_va, y_va, batch=batch, shuffle=False)\n",
        "\n",
        "        cb = [\n",
        "            keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=1, min_lr=1e-5),\n",
        "        ]\n",
        "        model.fit(tr_ds, validation_data=va_ds, epochs=epochs, verbose=0, callbacks=cb, class_weight=class_weight)\n",
        "\n",
        "        pred = model.predict(df_to_ds(cat_va, num_va, None, batch=batch), verbose=0).reshape(-1)\n",
        "        oof[va] = np.nan_to_num(pred, nan=0.5)\n",
        "\n",
        "    return roc_auc_score(y, oof)\n",
        "\n",
        "# build selected numeric matrices\n",
        "numA_sel = numA[selA].copy()\n",
        "numB_sel = numB[selB].copy()\n",
        "\n",
        "aucA = quick_oof(catA, numA_sel, y, epochs=12, splits=3, seed=42, batch=512)\n",
        "aucB = quick_oof(catB, numB_sel, y, epochs=12, splits=3, seed=42, batch=512)\n",
        "\n",
        "print(\"DL quick AUC A:\", aucA, \"DL quick AUC B:\", aucB)\n",
        "USE_A = (aucA >= aucB)\n",
        "print(\"✅ Selected option:\", \"A(reverse+mach)\" if USE_A else \"B(no reverse+mach)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 최종 학습 + 제출 (5-fold + seed ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seed OOF done: 42 AUC: 0.7617914482578695\n",
            "seed OOF done: 202 AUC: 0.7619252811040627\n",
            "seed OOF done: 777 AUC: 0.7610051715272811\n",
            "✅ Ensemble OOF AUC: 0.7633358963541882\n",
            "seed done: 42\n",
            "seed done: 202\n",
            "seed done: 777\n",
            "submission: (11383, 2) ['index', 'voted']\n",
            "count    11383.000000\n",
            "mean         0.525780\n",
            "std          0.235688\n",
            "min          0.116620\n",
            "25%          0.344356\n",
            "50%          0.434459\n",
            "75%          0.723689\n",
            "max          0.995433\n",
            "Name: voted, dtype: float64\n",
            "Saved: /Users/admin/Downloads/AI 헬스케어 수업/oz코딩 수업/해커톤 (1)/vote-AI/submission_best.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# final config\n",
        "FINAL_SEEDS = [42, 202, 777]\n",
        "EPOCHS_SUB = 25\n",
        "BATCH_SUB = 256\n",
        "N_SPLITS_SUB = 5\n",
        "\n",
        "# choose final data\n",
        "if USE_A:\n",
        "    cat_final = catA\n",
        "    num_final = numA[selA].copy()\n",
        "    tcat_final = tcatA\n",
        "    tnum_final = tnumA[selA].copy()\n",
        "else:\n",
        "    cat_final = catB\n",
        "    num_final = numB[selB].copy()\n",
        "    tcat_final = tcatB\n",
        "    tnum_final = tnumB[selB].copy()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS_SUB, shuffle=True, random_state=42)\n",
        "\n",
        "# OOF (optional, can be slow) — keep for sanity\n",
        "oof_by_seed = []\n",
        "for seed in FINAL_SEEDS:\n",
        "    oof = np.zeros(len(num_final), dtype=\"float32\")\n",
        "    for fold,(tr,va) in enumerate(skf.split(num_final, y)):\n",
        "        cat_tr, cat_va = cat_final.iloc[tr].reset_index(drop=True), cat_final.iloc[va].reset_index(drop=True)\n",
        "        num_tr, num_va = num_final.iloc[tr].reset_index(drop=True), num_final.iloc[va].reset_index(drop=True)\n",
        "        y_tr, y_va = y.iloc[tr].reset_index(drop=True), y.iloc[va].reset_index(drop=True)\n",
        "\n",
        "        pos=float(y_tr.mean()); neg=1.0-pos\n",
        "        class_weight={0:1.0, 1:neg/(pos+1e-9)}\n",
        "\n",
        "        tf.keras.utils.set_random_seed(seed+fold)\n",
        "        model = build_embed_mlp(cat_tr, num_tr)\n",
        "\n",
        "        tr_ds = df_to_ds(cat_tr, num_tr, y_tr, batch=BATCH_SUB, shuffle=True, seed=seed+fold)\n",
        "        va_ds = df_to_ds(cat_va, num_va, y_va, batch=BATCH_SUB, shuffle=False)\n",
        "\n",
        "        cb = [\n",
        "            keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=3, restore_best_weights=True),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=1, min_lr=1e-5),\n",
        "        ]\n",
        "        model.fit(tr_ds, validation_data=va_ds, epochs=EPOCHS_SUB, verbose=0, callbacks=cb, class_weight=class_weight)\n",
        "        pred = model.predict(df_to_ds(cat_va, num_va, None, batch=BATCH_SUB), verbose=0).reshape(-1)\n",
        "        oof[va] = np.nan_to_num(pred, nan=0.5)\n",
        "\n",
        "    oof_by_seed.append(oof)\n",
        "    print(\"seed OOF done:\", seed, \"AUC:\", roc_auc_score(y, oof))\n",
        "\n",
        "oof_ens = np.mean(np.vstack(oof_by_seed), axis=0)\n",
        "print(\"✅ Ensemble OOF AUC:\", roc_auc_score(y, oof_ens))\n",
        "\n",
        "# final train on full data per seed and predict test\n",
        "def train_full_predict(seed):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    model = build_embed_mlp(cat_final, num_final)\n",
        "\n",
        "    pos=float(y.mean()); neg=1.0-pos\n",
        "    class_weight={0:1.0, 1:neg/(pos+1e-9)}\n",
        "\n",
        "    tr_ds = df_to_ds(cat_final, num_final, y, batch=BATCH_SUB, shuffle=True, seed=seed)\n",
        "    model.fit(tr_ds, epochs=max(10, EPOCHS_SUB//2), verbose=0, class_weight=class_weight)\n",
        "\n",
        "    pred = model.predict(df_to_ds(tcat_final, tnum_final, None, batch=BATCH_SUB), verbose=0).reshape(-1)\n",
        "    return np.nan_to_num(pred, nan=0.5)\n",
        "\n",
        "preds = []\n",
        "for s in FINAL_SEEDS:\n",
        "    preds.append(train_full_predict(s))\n",
        "    print(\"seed done:\", s)\n",
        "\n",
        "pred_test = np.mean(np.vstack(preds), axis=0).reshape(-1)\n",
        "\n",
        "submission = sub.copy()\n",
        "submission[\"voted\"] = pred_test\n",
        "\n",
        "print(\"submission:\", submission.shape, submission.columns.tolist())\n",
        "print(submission[\"voted\"].describe())\n",
        "\n",
        "assert submission.shape == (11383, 2)\n",
        "assert submission.columns.tolist() == [\"index\",\"voted\"]\n",
        "assert float(submission[\"voted\"].min()) >= 0.0 and float(submission[\"voted\"].max()) <= 1.0\n",
        "\n",
        "out_path = PROJECT_ROOT / \"submission_best.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0b7de5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "voteai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
