{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_FTTransformer_v2_kfold 요약\n",
    "\n",
    "- 모델: FT-Transformer\n",
    "- 피처: v1 + missing indicator + qa_missing_ratio + race/religion 원본\n",
    "- 학습/평가: KFold 5, 누수 방지 전처리\n",
    "- 제출파일: submission_03_FTTransformer_v2_kfold.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dc932",
   "metadata": {},
   "source": [
    "# 04_third_test\n",
    "\n",
    "## 사용 피처\n",
    "- Q_A 태도: neg_att, pos_att, neutral_ratio, confident_ratio\n",
    "- Big5: diff/strength 10개\n",
    "- 단어 인지: wr_sum, wf_sum, word_credibility, cred_bin\n",
    "- 인구통계: age_group_ord, education, urban_ord\n",
    "- 무응답 indicator: education_is_missing, urban_is_missing, hand_is_missing, married_is_missing\n",
    "- QA missing: qa_missing_ratio\n",
    "- 범주형: hand_cat, married_cat, race_simple, religion_simple\n",
    "\n",
    "## 모델\n",
    "- FT-Transformer (CLS token 사용)\n",
    "- NumericalEmbedding + Categorical Embedding\n",
    "\n",
    "## 학습 세팅\n",
    "- Loss: BCEWithLogitsLoss(pos_weight)\n",
    "- Optimizer: AdamW\n",
    "- Scheduler: ReduceLROnPlateau (mode='max')\n",
    "- Early Stopping: val AUC 기준\n",
    "- Metric: ROC-AUC\n",
    "- 제출: 확률(sub['voted']=test_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd1f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363471af",
   "metadata": {},
   "source": [
    "## 0. 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ec43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 하이퍼파라미터\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "PATIENCE = 6\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 3e-4\n",
    "D_TOKEN = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b46d86",
   "metadata": {},
   "source": [
    "## 1. 유틸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80c488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def make_cfg(train_df):\n",
    "    cfg = {}\n",
    "    edu = pd.to_numeric(train_df[\"education\"], errors=\"coerce\").replace(0, np.nan)\n",
    "    cfg[\"education_mean\"] = float(edu.mean())\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def _normalize_cat(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.fillna(\"__MISSING__\")\n",
    "    s = s.replace({\"<NA>\": \"__MISSING__\", \"nan\": \"__MISSING__\", \"NaN\": \"__MISSING__\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "def _fit_cat_maps(df_train, cat_cols):\n",
    "    cat_maps = {}\n",
    "    for col in cat_cols:\n",
    "        s = _normalize_cat(df_train[col])\n",
    "        if \"__MISSING__\" not in s.unique():\n",
    "            s = pd.concat([s, pd.Series([\"__MISSING__\"])], ignore_index=True)\n",
    "        cats = sorted(s.astype(str).unique().tolist())\n",
    "        cat_maps[col] = {v: i for i, v in enumerate(cats)}\n",
    "    return cat_maps\n",
    "\n",
    "\n",
    "def _transform_cat(series, mapping):\n",
    "    s = _normalize_cat(series)\n",
    "    return s.apply(lambda x: mapping.get(x, mapping.get(\"__MISSING__\"))).astype(int).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284a308",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d571c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "train_raw = pd.read_csv(\"../../data/raw/train.csv\")\n",
    "test_raw  = pd.read_csv(\"../../data/raw/test_x.csv\")\n",
    "sample_sub = pd.read_csv(\"../../data/raw/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd0394",
   "metadata": {},
   "source": [
    "## 3. build_features (요구사항 반영)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f19ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df_raw, cfg=None, is_train=True):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    if is_train and \"voted\" in df.columns:\n",
    "        df[\"voted_bin\"] = (df[\"voted\"] == 2).astype(int)\n",
    "\n",
    "    if \"age_group\" in df.columns:\n",
    "        age_map = {\"10s\":1, \"20s\":2, \"30s\":3, \"40s\":4, \"50s\":5, \"60s\":6, \"+70s\":7}\n",
    "        df[\"age_group_ord\"] = df[\"age_group\"].map(age_map).astype(\"float32\")\n",
    "\n",
    "    if \"education\" in df.columns:\n",
    "        df[\"education\"] = pd.to_numeric(df[\"education\"], errors=\"coerce\")\n",
    "        df.loc[df[\"education\"] == 0, \"education\"] = np.nan\n",
    "        df[\"education_is_missing\"] = df[\"education\"].isna().astype(\"float32\")\n",
    "        if cfg is not None:\n",
    "            df[\"education\"] = df[\"education\"].fillna(cfg[\"education_mean\"])\n",
    "        df[\"education\"] = df[\"education\"].astype(\"float32\")\n",
    "\n",
    "    if \"married\" in df.columns:\n",
    "        df[\"married\"] = pd.to_numeric(df[\"married\"], errors=\"coerce\")\n",
    "        df.loc[df[\"married\"] == 0, \"married\"] = np.nan\n",
    "        df[\"married_is_missing\"] = df[\"married\"].isna().astype(\"float32\")\n",
    "        df[\"married_cat\"] = df[\"married\"].astype(\"string\")\n",
    "\n",
    "    if \"hand\" in df.columns:\n",
    "        df[\"hand\"] = pd.to_numeric(df[\"hand\"], errors=\"coerce\")\n",
    "        df.loc[df[\"hand\"] == 0, \"hand\"] = np.nan\n",
    "        df[\"hand_is_missing\"] = df[\"hand\"].isna().astype(\"float32\")\n",
    "        df[\"hand_cat\"] = df[\"hand\"].astype(\"string\")\n",
    "\n",
    "    if \"urban\" in df.columns:\n",
    "        df[\"urban\"] = pd.to_numeric(df[\"urban\"], errors=\"coerce\")\n",
    "        df.loc[df[\"urban\"] == 0, \"urban\"] = np.nan\n",
    "        df[\"urban_is_missing\"] = df[\"urban\"].isna().astype(\"float32\")\n",
    "        df[\"urban_ord\"] = df[\"urban\"].astype(\"float32\")\n",
    "\n",
    "    neg_cols = [\"QbA\",\"QcA\",\"QjA\",\"QmA\",\"QoA\",\"QsA\"]\n",
    "    pos_cols = [\"QkA\",\"QqA\"]\n",
    "    other_cols = [\"QeA\",\"QfA\",\"QhA\",\"QrA\"]\n",
    "\n",
    "    qa_cols = [c for c in df.columns if c.startswith(\"Q\") and c.endswith(\"A\")]\n",
    "\n",
    "    for col in qa_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    if all(c in df.columns for c in neg_cols):\n",
    "        df[\"neg_att\"] = df[neg_cols].mean(axis=1)\n",
    "\n",
    "    if all(c in df.columns for c in pos_cols):\n",
    "        df[\"pos_att\"] = df[pos_cols].mean(axis=1)\n",
    "\n",
    "    if all(c in df.columns for c in other_cols):\n",
    "        other = df[other_cols]\n",
    "        denom = other.notna().sum(axis=1)\n",
    "        df[\"neutral_ratio\"] = np.where(denom > 0, (other == 3).sum(axis=1) / denom, np.nan).astype(\"float32\")\n",
    "        df[\"confident_ratio\"] = np.where(denom > 0, other.isin([1,2,4,5]).sum(axis=1) / denom, np.nan).astype(\"float32\")\n",
    "\n",
    "    if len(qa_cols) > 0:\n",
    "        qa_mat = df[qa_cols]\n",
    "        df[\"qa_missing_ratio\"] = qa_mat.isna().sum(axis=1) / len(qa_cols)\n",
    "\n",
    "    tp_pairs = {\n",
    "        \"extraversion\": (\"tp01\", \"tp06\"),\n",
    "        \"agreeableness\": (\"tp07\", \"tp02\"),\n",
    "        \"conscientiousness\": (\"tp03\", \"tp08\"),\n",
    "        \"neuroticism\": (\"tp04\", \"tp09\"),\n",
    "        \"openness\": (\"tp05\", \"tp10\"),\n",
    "    }\n",
    "    for trait, (a, b) in tp_pairs.items():\n",
    "        if a in df.columns and b in df.columns:\n",
    "            df[a] = pd.to_numeric(df[a], errors=\"coerce\")\n",
    "            df[b] = pd.to_numeric(df[b], errors=\"coerce\")\n",
    "            df[f\"{trait}_diff\"] = (df[a] - df[b]).astype(\"float32\")\n",
    "            df[f\"{trait}_strength\"] = df[f\"{trait}_diff\"].abs().astype(\"float32\")\n",
    "\n",
    "    wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "\n",
    "    if all(c in df.columns for c in wr_cols):\n",
    "        df[\"wr_sum\"] = df[wr_cols].sum(axis=1).astype(\"float32\")\n",
    "\n",
    "    if all(c in df.columns for c in wf_cols):\n",
    "        df[\"wf_sum\"] = df[wf_cols].sum(axis=1).astype(\"float32\")\n",
    "\n",
    "    if \"wr_sum\" in df.columns and \"wf_sum\" in df.columns:\n",
    "        df[\"word_credibility\"] = (df[\"wr_sum\"] - df[\"wf_sum\"]).astype(\"float32\")\n",
    "\n",
    "    if \"word_credibility\" in df.columns:\n",
    "        df[\"cred_bin\"] = pd.cut(\n",
    "            df[\"word_credibility\"],\n",
    "            bins=[-3, 1, 6, 13],\n",
    "            labels=[\"Low\", \"Mid\", \"High\"]\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df92c9",
   "metadata": {},
   "source": [
    "## 4. 컬럼 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"voted_bin\"\n",
    "FEATURE_COLS = [\n",
    "    \"neg_att\", \"pos_att\", \"neutral_ratio\", \"confident_ratio\",\n",
    "    \"extraversion_diff\", \"extraversion_strength\",\n",
    "    \"agreeableness_diff\", \"agreeableness_strength\",\n",
    "    \"conscientiousness_diff\", \"conscientiousness_strength\",\n",
    "    \"neuroticism_diff\", \"neuroticism_strength\",\n",
    "    \"openness_diff\", \"openness_strength\",\n",
    "    \"wr_sum\", \"wf_sum\", \"word_credibility\", \"cred_bin\",\n",
    "    \"age_group_ord\", \"education\", \"urban_ord\",\n",
    "    \"education_is_missing\", \"urban_is_missing\", \"hand_is_missing\", \"married_is_missing\",\n",
    "    \"qa_missing_ratio\",\n",
    "    \"hand_cat\", \"married_cat\",\n",
    "    \"race\", \"religion\",\n",
    "]\n",
    "\n",
    "CAT_COLS = [\"race\", \"religion\", \"cred_bin\", \"hand_cat\", \"married_cat\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381565d",
   "metadata": {},
   "source": [
    "## 5. 전처리 (누수 방지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318ca70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(train_df, val_df, test_df, feature_cols, cat_cols, target):\n",
    "    y_train = train_df[target].values\n",
    "    y_val = val_df[target].values\n",
    "\n",
    "    cat_maps = _fit_cat_maps(train_df, cat_cols)\n",
    "\n",
    "    X_cat_train = np.stack([_transform_cat(train_df[c], cat_maps[c]) for c in cat_cols], axis=1)\n",
    "    X_cat_val   = np.stack([_transform_cat(val_df[c], cat_maps[c])   for c in cat_cols], axis=1)\n",
    "    X_cat_test  = np.stack([_transform_cat(test_df[c], cat_maps[c])  for c in cat_cols], axis=1)\n",
    "\n",
    "    num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "    train_num = train_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    val_num   = val_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test_num  = test_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    train_means = train_num.mean()\n",
    "    train_num = train_num.fillna(train_means)\n",
    "    val_num   = val_num.fillna(train_means)\n",
    "    test_num  = test_num.fillna(train_means)\n",
    "\n",
    "    BINARY_COLS = [\n",
    "        \"education_is_missing\",\"urban_is_missing\",\"hand_is_missing\",\"married_is_missing\"\n",
    "    ]\n",
    "    RATIO_COLS = [\"qa_missing_ratio\",\"neutral_ratio\",\"confident_ratio\"]\n",
    "\n",
    "    pass_cols = [c for c in (BINARY_COLS + RATIO_COLS) if c in num_cols]\n",
    "    scale_cols = [c for c in num_cols if c not in pass_cols]\n",
    "\n",
    "    # 스케일링은 continuous만\n",
    "    if len(scale_cols) > 0:\n",
    "        scaler = StandardScaler()\n",
    "        X_num_train_scaled = scaler.fit_transform(train_num[scale_cols].values)\n",
    "        X_num_val_scaled   = scaler.transform(val_num[scale_cols].values)\n",
    "        X_num_test_scaled  = scaler.transform(test_num[scale_cols].values)\n",
    "    else:\n",
    "        scaler = None\n",
    "        X_num_train_scaled = np.zeros((len(train_num), 0))\n",
    "        X_num_val_scaled   = np.zeros((len(val_num), 0))\n",
    "        X_num_test_scaled  = np.zeros((len(test_num), 0))\n",
    "\n",
    "    # pass-through (0/1, ratio)\n",
    "    if len(pass_cols) > 0:\n",
    "        X_num_train_pass = train_num[pass_cols].values\n",
    "        X_num_val_pass   = val_num[pass_cols].values\n",
    "        X_num_test_pass  = test_num[pass_cols].values\n",
    "    else:\n",
    "        X_num_train_pass = np.zeros((len(train_num), 0))\n",
    "        X_num_val_pass   = np.zeros((len(val_num), 0))\n",
    "        X_num_test_pass  = np.zeros((len(test_num), 0))\n",
    "\n",
    "    X_num_train = np.concatenate([X_num_train_scaled, X_num_train_pass], axis=1)\n",
    "    X_num_val   = np.concatenate([X_num_val_scaled,   X_num_val_pass], axis=1)\n",
    "    X_num_test  = np.concatenate([X_num_test_scaled,  X_num_test_pass], axis=1)\n",
    "\n",
    "    cat_dims = [len(cat_maps[c]) for c in cat_cols]\n",
    "\n",
    "    return {\n",
    "        \"X_num_train\": X_num_train,\n",
    "        \"X_num_val\": X_num_val,\n",
    "        \"X_num_test\": X_num_test,\n",
    "        \"X_cat_train\": X_cat_train,\n",
    "        \"X_cat_val\": X_cat_val,\n",
    "        \"X_cat_test\": X_cat_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"cat_dims\": cat_dims,\n",
    "        \"num_cols\": num_cols,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ae167",
   "metadata": {},
   "source": [
    "## 6. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04f7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f1749",
   "metadata": {},
   "source": [
    "## 7. FT-Transformer (CLS token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb76897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features, d_token))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, num_features, d_token=D_TOKEN, n_layers=N_LAYERS,\n",
    "                 n_heads=N_HEADS, dropout=DROPOUT, attn_dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(dim, d_token) for dim in cat_dims])\n",
    "        self.num_embed = NumericalEmbedding(num_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_token * 4,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        cat_tokens = []\n",
    "        for i, emb in enumerate(self.cat_embeds):\n",
    "            cat_tokens.append(emb(x_cat[:, i]))\n",
    "        cat_tokens = torch.stack(cat_tokens, dim=1) if len(cat_tokens) > 0 else None\n",
    "\n",
    "        num_tokens = self.num_embed(x_num)\n",
    "        tokens = num_tokens if cat_tokens is None else torch.cat([cat_tokens, num_tokens], dim=1)\n",
    "\n",
    "        cls = self.cls_token.expand(tokens.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        x = self.transformer(tokens)\n",
    "        cls_out = x[:, 0]\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518b4dc",
   "metadata": {},
   "source": [
    "## 8. 학습/예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2b4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(X_num_train, X_cat_train, y_train, X_num_val, X_cat_val, y_val, cat_dims, num_features):\n",
    "    train_ds = TabDataset(X_num_train, X_cat_train, y_train)\n",
    "    val_ds = TabDataset(X_num_val, X_cat_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = FTTransformer(cat_dims=cat_dims, num_features=num_features).to(DEVICE)\n",
    "\n",
    "    pos_weight = (len(y_train) - y_train.sum()) / (y_train.sum() + 1e-6)\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb_num, xb_cat, yb in train_loader:\n",
    "            xb_num = xb_num.to(DEVICE)\n",
    "            xb_cat = xb_cat.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb_num, xb_cat, yb in val_loader:\n",
    "                xb_num = xb_num.to(DEVICE)\n",
    "                xb_cat = xb_cat.to(DEVICE)\n",
    "                logits = model(xb_num, xb_cat)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "                val_probs.append(probs)\n",
    "\n",
    "        val_probs = np.concatenate(val_probs)\n",
    "        val_auc = roc_auc_score(y_val, val_probs)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-4:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1} | val AUC {val_auc:.4f}\")\n",
    "\n",
    "        if patience_ctr >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_auc\n",
    "\n",
    "\n",
    "def predict_proba(model, X_num, X_cat):\n",
    "    ds = TabDataset(X_num, X_cat, y=None)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat in loader:\n",
    "            xb_num = xb_num.to(DEVICE)\n",
    "            xb_cat = xb_cat.to(DEVICE)\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            p = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            probs.append(p)\n",
    "    return np.concatenate(probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. K-Fold 학습 및 제출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1]\n",
      "Epoch 5 | val AUC 0.7716\n",
      "Epoch 10 | val AUC 0.7732\n",
      "Epoch 15 | val AUC 0.7744\n",
      "Epoch 20 | val AUC 0.7737\n",
      "Fold 1 best AUC: 0.7744\n",
      "[Fold 2]\n",
      "Epoch 5 | val AUC 0.7620\n",
      "Epoch 10 | val AUC 0.7644\n",
      "Epoch 15 | val AUC 0.7657\n",
      "Epoch 20 | val AUC 0.7659\n",
      "Fold 2 best AUC: 0.7663\n",
      "[Fold 3]\n",
      "Epoch 5 | val AUC 0.7573\n",
      "Epoch 10 | val AUC 0.7585\n",
      "Epoch 15 | val AUC 0.7595\n",
      "Fold 3 best AUC: 0.7605\n",
      "[Fold 4]\n",
      "Epoch 5 | val AUC 0.7550\n",
      "Epoch 10 | val AUC 0.7554\n",
      "Epoch 15 | val AUC 0.7570\n",
      "Fold 4 best AUC: 0.7572\n",
      "[Fold 5]\n",
      "Epoch 5 | val AUC 0.7618\n",
      "Epoch 10 | val AUC 0.7656\n",
      "Epoch 15 | val AUC 0.7656\n",
      "Fold 5 best AUC: 0.7656\n",
      "Mean Validation ROC-AUC: 0.7648\n",
      "saved submission_ft_third_test_prob_class2.csv\n"
     ]
    }
   ],
   "source": [
    "y_all = (train_raw[\"voted\"] == 2).astype(int).values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_aucs = []\n",
    "test_probs_list = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(train_raw, y_all), 1):\n",
    "    print(f\"[Fold {fold}]\")\n",
    "    tr_raw = train_raw.iloc[tr_idx].copy()\n",
    "    va_raw = train_raw.iloc[va_idx].copy()\n",
    "\n",
    "    cfg = make_cfg(tr_raw)\n",
    "\n",
    "    tr_feat = build_features(tr_raw, cfg=cfg, is_train=True)\n",
    "    va_feat = build_features(va_raw, cfg=cfg, is_train=True)\n",
    "    te_feat = build_features(test_raw, cfg=cfg, is_train=False)\n",
    "\n",
    "    prep = fit_transform(tr_feat, va_feat, te_feat, FEATURE_COLS, CAT_COLS, TARGET)\n",
    "\n",
    "    num_features = prep[\"X_num_train\"].shape[1]\n",
    "\n",
    "    model, best_auc = train_one_fold(\n",
    "        prep[\"X_num_train\"], prep[\"X_cat_train\"], prep[\"y_train\"],\n",
    "        prep[\"X_num_val\"], prep[\"X_cat_val\"], prep[\"y_val\"],\n",
    "        prep[\"cat_dims\"], num_features\n",
    ")\n",
    "\n",
    "\n",
    "    fold_aucs.append(best_auc)\n",
    "    test_probs = predict_proba(model, prep[\"X_num_test\"], prep[\"X_cat_test\"])\n",
    "    test_probs_list.append(test_probs)\n",
    "\n",
    "    print(f\"Fold {fold} best AUC: {best_auc:.4f}\")\n",
    "\n",
    "mean_auc = float(np.mean(fold_aucs))\n",
    "print(f\"Mean Validation ROC-AUC: {mean_auc:.4f}\")\n",
    "\n",
    "# test probs average\n",
    "test_probs_mean = np.mean(np.stack(test_probs_list, axis=0), axis=0)\n",
    "\n",
    "# 제출 생성 (index 매핑 방식)\n",
    "test_pred = pd.DataFrame({\n",
    "    \"index\": test_raw[\"index\"].values,\n",
    "    \"voted\": test_probs_mean\n",
    "})\n",
    "\n",
    "sub = sample_sub.drop(columns=[\"voted\"]).merge(test_pred, on=\"index\", how=\"left\")\n",
    "sub = sub[[\"index\", \"voted\"]].sort_values(\"index\").reset_index(drop=True)\n",
    "sub[\"voted\"] = sub[\"voted\"].astype(float)\n",
    "\n",
    "assert sub[\"voted\"].isna().sum() == 0, \"NaN in submission\"\n",
    "\n",
    "sub.to_csv(\"submission_03_FTTransformer_v2_kfold.csv\", index=False)\n",
    "print(\"saved submission_03_FTTransformer_v2_kfold.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}