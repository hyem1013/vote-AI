{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16_v5_ResMLP_v1 ÏöîÏïΩ\n",
    "\n",
    "- Î™®Îç∏: v5 Ïã§Ìóò(ResMLP/FT/ÏïôÏÉÅÎ∏î ÎπÑÍµê)\n",
    "- ÌîºÏ≤ò: v5 ÏÑ∏Ìä∏ (TE_ONLY/RAW_ONLY/SUMMARY_ONLY ÎπÑÍµê)\n",
    "- ÌïôÏäµ/ÌèâÍ∞Ä: KFold 5\n",
    "- Ï†úÏ∂úÌååÏùº: (Ïã§Ìñâ ÏòµÏÖòÏóê Îî∞Îùº ÏÉùÏÑ±)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cpu\n",
      "üìÇ TRAIN_PATH: ../../data/raw/train.csv\n",
      "üìÇ TEST_PATH : ../../data/raw/test_x.csv\n",
      "Train: (45532, 79), Test: (11383, 77)\n",
      "\n",
      "================================================================================\n",
      "üß™ EXP: S0_demo_te\n",
      "   groups: ['demo_base', 'demo_derived', 'te']\n",
      "================================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "   MLP fold AUC: 0.77746\n",
      "   FT  fold AUC: 0.77706\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "   MLP fold AUC: 0.76696\n",
      "   FT  fold AUC: 0.76664\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "   MLP fold AUC: 0.76388\n",
      "   FT  fold AUC: 0.76215\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "   MLP fold AUC: 0.75928\n",
      "   FT  fold AUC: 0.75786\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "   MLP fold AUC: 0.76440\n",
      "   FT  fold AUC: 0.76223\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OOF AUC - MLP : 0.76606  | folds: ['0.77746', '0.76696', '0.76388', '0.75928', '0.76440']\n",
      "OOF AUC - FT  : 0.76299  | folds: ['0.77706', '0.76664', '0.76215', '0.75786', '0.76223']\n",
      "OOF AUC - AVG : 0.76600\n",
      "OOF AUC - BEST WEIGHT: w=0.7 (MLP) + 0.3 (FT) => 0.76641\n",
      "--------------------------------------------------------------------------------\n",
      "üíæ saved: outputs_v5_ablation/sub_v5exp-S0_demo_te_oof-0.76641_seed42_20260130-1239.csv\n",
      "   pred range: [0.1632, 0.9938]\n",
      "\n",
      "================================================================================\n",
      "üß™ EXP: S1_demo_te_wr_tp\n",
      "   groups: ['demo_base', 'demo_derived', 'te', 'wr_raw', 'wf_raw', 'wr_derived', 'tp_raw', 'tp_derived', 'interactions']\n",
      "================================================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "   MLP fold AUC: 0.78093\n",
      "   FT  fold AUC: 0.77958\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "   MLP fold AUC: 0.76812\n",
      "   FT  fold AUC: 0.77017\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "   MLP fold AUC: 0.76226\n",
      "   FT  fold AUC: 0.75818\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "   MLP fold AUC: 0.76079\n",
      "   FT  fold AUC: 0.76130\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "   MLP fold AUC: 0.77028\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "v5 Feature Ablation Runner\n",
    "- Keep v5 models (MLP + FTTransformer) & training logic\n",
    "- Toggle feature groups (raw/derived/TE/etc.) per experiment\n",
    "- Auto-naming submission files\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "PATIENCE = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Device: {DEVICE}\")\n",
    "\n",
    "# Path priority: local -> repo default -> /mnt/data fallback\n",
    "CANDIDATE_TRAIN = [\"train.csv\", \"../../data/raw/train.csv\", \"/mnt/data/train.csv\"]\n",
    "CANDIDATE_TEST  = [\"test_x.csv\", \"../../data/raw/test_x.csv\", \"/mnt/data/test_x.csv\"]\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"None of these paths exist: {paths}\")\n",
    "\n",
    "TRAIN_PATH = pick_existing(CANDIDATE_TRAIN)\n",
    "TEST_PATH  = pick_existing(CANDIDATE_TEST)\n",
    "print(f\"üìÇ TRAIN_PATH: {TRAIN_PATH}\")\n",
    "print(f\"üìÇ TEST_PATH : {TEST_PATH}\")\n",
    "\n",
    "OUTDIR = \"outputs_v5_ablation\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load data\n",
    "# ============================================================\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_raw[\"voted_bin\"] = (train_raw[\"voted\"] == 2).astype(int)\n",
    "print(f\"Train: {train_raw.shape}, Test: {test_raw.shape}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Cleaning\n",
    "# ============================================================\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # no-answer (0 -> NaN)\n",
    "    for col in [\"education\", \"engnat\", \"hand\", \"married\", \"urban\"]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    # familysize\n",
    "    if \"familysize\" in df.columns:\n",
    "        df.loc[df[\"familysize\"] == 0, \"familysize\"] = np.nan\n",
    "        df.loc[df[\"familysize\"] > 15, \"familysize\"] = np.nan\n",
    "\n",
    "    # TP 0 -> NaN\n",
    "    for col in [f\"tp{i:02d}\" for i in range(1, 11)]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    # Q_E clipping\n",
    "    for col in [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(lower=100, upper=60000)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature Engineering (same spirit as your v5)\n",
    "# ============================================================\n",
    "def build_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # demographics\n",
    "    age_map = {\"10s\": 1, \"20s\": 2, \"30s\": 3, \"40s\": 4, \"50s\": 5, \"60s\": 6, \"+70s\": 7}\n",
    "    df[\"age_ord\"] = df[\"age_group\"].map(age_map)\n",
    "    df[\"is_teenager\"] = (df[\"age_ord\"] == 1).astype(int)\n",
    "    df[\"is_young\"] = (df[\"age_ord\"] <= 2).astype(int)\n",
    "    df[\"is_old\"] = (df[\"age_ord\"] >= 6).astype(int)\n",
    "    df[\"edu_low\"] = (df[\"education\"] <= 2).astype(float)\n",
    "    df[\"edu_high\"] = (df[\"education\"] >= 3).astype(float)\n",
    "    df[\"is_single\"] = (df[\"married\"] == 1).astype(float)\n",
    "    df[\"is_married\"] = (df[\"married\"] == 2).astype(float)\n",
    "    df[\"is_urban\"] = (df[\"urban\"] == 3).astype(float)\n",
    "    df[\"is_english_native\"] = (df[\"engnat\"] == 1).astype(float)\n",
    "    df[\"is_male\"] = (df[\"gender\"] == \"Male\").astype(int)\n",
    "\n",
    "    # Q_A raw + aggregates\n",
    "    qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    df[\"qa_mean\"] = df[qa_cols].mean(axis=1)\n",
    "    df[\"qa_std\"] = df[qa_cols].std(axis=1)\n",
    "    df[\"qa_range\"] = df[qa_cols].max(axis=1) - df[qa_cols].min(axis=1)\n",
    "    df[\"qa_extreme_ratio\"] = ((df[qa_cols] == 1) | (df[qa_cols] == 5)).sum(axis=1) / 20\n",
    "    df[\"qa_neutral_ratio\"] = (df[qa_cols] == 3).sum(axis=1) / 20\n",
    "    df[\"qa_all_same\"] = (df[qa_cols].std(axis=1) == 0).astype(int)\n",
    "\n",
    "    # Q_E log + aggregates\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    for col in qe_cols:\n",
    "        df[f\"{col}_log\"] = np.log1p(df[col])\n",
    "\n",
    "    qe_log_cols = [f\"{col}_log\" for col in qe_cols]\n",
    "    df[\"qe_log_mean\"] = df[qe_log_cols].mean(axis=1)\n",
    "    df[\"qe_log_std\"] = df[qe_log_cols].std(axis=1)\n",
    "    df[\"qe_fast_ratio\"] = (df[qe_cols] < 500).sum(axis=1) / 20\n",
    "    df[\"qe_total_log\"] = df[qe_log_cols].sum(axis=1)\n",
    "    df[\"is_careless\"] = ((df[qe_cols].mean(axis=1) < 500) | (df[\"qa_all_same\"] == 1)).astype(int)\n",
    "\n",
    "    # TP Big5\n",
    "    tp_cols = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "    df[\"tp_missing_ratio\"] = df[tp_cols].isna().sum(axis=1) / 10\n",
    "    df[\"extraversion\"] = df[\"tp01\"] - df[\"tp06\"]\n",
    "    df[\"agreeableness\"] = df[\"tp07\"] - df[\"tp02\"]\n",
    "    df[\"conscientiousness\"] = df[\"tp03\"] - df[\"tp08\"]\n",
    "    df[\"neuroticism\"] = df[\"tp04\"] - df[\"tp09\"]\n",
    "    df[\"openness\"] = df[\"tp05\"] - df[\"tp10\"]\n",
    "    df[\"tp_mean\"] = df[tp_cols].mean(axis=1)\n",
    "\n",
    "    # WR/WF\n",
    "    wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "    df[\"wr_sum\"] = df[wr_cols].sum(axis=1)\n",
    "    df[\"wf_sum\"] = df[wf_cols].sum(axis=1)\n",
    "    df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
    "    df[\"vocab_high\"] = (df[\"wr_sum\"] >= 11).astype(int)\n",
    "\n",
    "    # interactions\n",
    "    df[\"age_edu\"] = df[\"age_ord\"] * df[\"education\"]\n",
    "    df[\"young_low_edu\"] = df[\"is_young\"] * df[\"edu_low\"]\n",
    "    df[\"young_single\"] = df[\"is_young\"] * df[\"is_single\"]\n",
    "    df[\"old_married\"] = df[\"is_old\"] * df[\"is_married\"]\n",
    "    df[\"teenager_low_edu\"] = df[\"is_teenager\"] * df[\"edu_low\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Target Encoding (fold-safe: fit on train_fold only)\n",
    "# ============================================================\n",
    "def target_encode(train_df, val_df, test_df, col, target_col, smoothing=10):\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    agg = train_df.groupby(col)[target_col].agg([\"mean\", \"count\"])\n",
    "    agg[\"te\"] = (agg[\"count\"] * agg[\"mean\"] + smoothing * global_mean) / (agg[\"count\"] + smoothing)\n",
    "    te_map = agg[\"te\"].to_dict()\n",
    "\n",
    "    tr = train_df[col].map(te_map).fillna(global_mean).values\n",
    "    va = val_df[col].map(te_map).fillna(global_mean).values\n",
    "    te = test_df[col].map(te_map).fillna(global_mean).values\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "def create_target_encodings(train_df, val_df, test_df, target_col=\"voted_bin\"):\n",
    "    te = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "    # single\n",
    "    for col in [\"age_group\", \"race\", \"religion\"]:\n",
    "        tr, va, tt = target_encode(train_df, val_df, test_df, col, target_col, smoothing=10)\n",
    "        te[\"train\"][f\"{col}_te\"] = tr\n",
    "        te[\"val\"][f\"{col}_te\"] = va\n",
    "        te[\"test\"][f\"{col}_te\"] = tt\n",
    "\n",
    "    # composite (must be created on all dfs)\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[\"age_edu_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"education\"].astype(str)\n",
    "        df[\"age_married_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"married\"].astype(str)\n",
    "        df[\"age_race_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"race\"].astype(str)\n",
    "        df[\"age_edu_married_cat\"] = (\n",
    "            df[\"age_group\"].astype(str) + \"_\" + df[\"education\"].astype(str) + \"_\" + df[\"married\"].astype(str)\n",
    "        )\n",
    "\n",
    "    for col, sm in [\n",
    "        (\"age_edu_cat\", 5),\n",
    "        (\"age_married_cat\", 5),\n",
    "        (\"age_race_cat\", 5),\n",
    "        (\"age_edu_married_cat\", 3),\n",
    "    ]:\n",
    "        tr, va, tt = target_encode(train_df, val_df, test_df, col, target_col, smoothing=sm)\n",
    "        te[\"train\"][f\"{col}_te\"] = tr\n",
    "        te[\"val\"][f\"{col}_te\"] = va\n",
    "        te[\"test\"][f\"{col}_te\"] = tt\n",
    "\n",
    "    return te\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Column pools (raw + derived)\n",
    "# ============================================================\n",
    "QA_RAW = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "QE_RAW = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "QE_LOG = [f\"Q{c}E_log\" for c in \"abcdefghijklmnopqrst\"]\n",
    "TP_RAW = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "WR_RAW = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "WF_RAW = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "\n",
    "DEMO_BASE = [\"age_ord\", \"education\", \"married\", \"urban\", \"engnat\", \"familysize\", \"hand\"]\n",
    "DEMO_DERIVED = [\n",
    "    \"is_teenager\", \"is_young\", \"is_old\",\n",
    "    \"edu_low\", \"edu_high\",\n",
    "    \"is_single\", \"is_married\", \"is_urban\",\n",
    "    \"is_english_native\", \"is_male\",\n",
    "]\n",
    "\n",
    "QA_DERIVED = [\"qa_mean\", \"qa_std\", \"qa_range\", \"qa_extreme_ratio\", \"qa_neutral_ratio\", \"qa_all_same\"]\n",
    "QE_DERIVED = [\"qe_log_mean\", \"qe_log_std\", \"qe_fast_ratio\", \"qe_total_log\", \"is_careless\"]\n",
    "TP_DERIVED = [\"tp_missing_ratio\", \"tp_mean\", \"extraversion\", \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\"]\n",
    "WR_DERIVED = [\"wr_sum\", \"wf_sum\", \"word_credibility\", \"vocab_high\"]\n",
    "INTERACTIONS = [\"age_edu\", \"young_low_edu\", \"young_single\", \"old_married\", \"teenager_low_edu\"]\n",
    "\n",
    "TE_FEATURES = [\n",
    "    \"age_group_te\", \"race_te\", \"religion_te\",\n",
    "    \"age_edu_cat_te\", \"age_married_cat_te\", \"age_race_cat_te\", \"age_edu_married_cat_te\",\n",
    "]\n",
    "\n",
    "CAT_FEATURES = [\"gender\", \"race\", \"religion\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature Groups (switchable)\n",
    "# ============================================================\n",
    "FEATURE_GROUPS = {\n",
    "    # numeric groups\n",
    "    \"demo_base\": DEMO_BASE,\n",
    "    \"demo_derived\": DEMO_DERIVED,\n",
    "    \"qa_raw\": QA_RAW,\n",
    "    \"qa_derived\": QA_DERIVED,\n",
    "    \"qe_log\": QE_LOG,          # log versions only\n",
    "    \"qe_derived\": QE_DERIVED,\n",
    "    \"tp_raw\": TP_RAW,\n",
    "    \"tp_derived\": TP_DERIVED,\n",
    "    \"wr_raw\": WR_RAW,\n",
    "    \"wf_raw\": WF_RAW,\n",
    "    \"wr_derived\": WR_DERIVED,\n",
    "    \"interactions\": INTERACTIONS,\n",
    "    \"te\": TE_FEATURES,\n",
    "}\n",
    "\n",
    "# categorical group (always used by embedding)\n",
    "CAT_GROUP = {\"cat_basic\": CAT_FEATURES}\n",
    "\n",
    "\n",
    "def resolve_feature_list(include_groups):\n",
    "    cols = []\n",
    "    for g in include_groups:\n",
    "        cols.extend(FEATURE_GROUPS[g])\n",
    "    # de-dup but keep order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            out.append(c)\n",
    "            seen.add(c)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Experiments (edit here)\n",
    "# ============================================================\n",
    "EXPERIMENTS = {\n",
    "    # Baselines\n",
    "    \"S0_demo_te\": [\"demo_base\", \"demo_derived\", \"te\"],\n",
    "    \"S1_demo_te_wr_tp\": [\"demo_base\", \"demo_derived\", \"te\", \"wr_raw\", \"wf_raw\", \"wr_derived\", \"tp_raw\", \"tp_derived\", \"interactions\"],\n",
    "\n",
    "    # Q_A tests\n",
    "    \"S2_add_QA_derived\": [\"demo_base\", \"demo_derived\", \"te\", \"wr_raw\", \"wf_raw\", \"wr_derived\", \"tp_raw\", \"tp_derived\", \"interactions\", \"qa_derived\"],\n",
    "    \"S3_add_QA_raw20\":   [\"demo_base\", \"demo_derived\", \"te\", \"wr_raw\", \"wf_raw\", \"wr_derived\", \"tp_raw\", \"tp_derived\", \"interactions\", \"qa_raw\"],\n",
    "\n",
    "    # Q_E tests\n",
    "    \"S4_add_QE_derived\": [\"demo_base\", \"demo_derived\", \"te\", \"wr_raw\", \"wf_raw\", \"wr_derived\", \"tp_raw\", \"tp_derived\", \"interactions\", \"qe_derived\"],\n",
    "    \"S5_add_QE_log20\":   [\"demo_base\", \"demo_derived\", \"te\", \"wr_raw\", \"wf_raw\", \"wr_derived\", \"tp_raw\", \"tp_derived\", \"interactions\", \"qe_log\"],\n",
    "\n",
    "    # Full-ish (your v5 spirit)\n",
    "    \"S6_full_v5like\": [\n",
    "        \"demo_base\", \"demo_derived\", \"te\",\n",
    "        \"qa_raw\", \"qa_derived\",\n",
    "        \"qe_log\", \"qe_derived\",\n",
    "        \"tp_raw\", \"tp_derived\",\n",
    "        \"wr_raw\", \"wf_raw\", \"wr_derived\",\n",
    "        \"interactions\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model 1: MLP\n",
    "# ============================================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, embed_dim=8, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim + 1, embed_dim) for dim in cat_dims])\n",
    "\n",
    "        input_dim = num_features + len(cat_dims) * embed_dim\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_dims[-1], 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        cat_embeds = torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=1)\n",
    "        x = torch.cat([x_num, cat_embeds], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Model 2: FT-Transformer (light)\n",
    "# ============================================================\n",
    "class NumericalEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, d_token) * 0.02)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features, d_token))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, d_token=48, n_layers=2, n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_embed = NumericalEmbedding(num_features, d_token)\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(dim + 1, d_token) for dim in cat_dims])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token, nhead=n_heads, dim_feedforward=d_token * 2,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        num_tokens = self.num_embed(x_num)  # (B, n_num, d)\n",
    "        cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)], dim=1)  # (B, n_cat, d)\n",
    "\n",
    "        tokens = torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "        cls = self.cls_token.expand(tokens.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        x = self.transformer(tokens)\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Train / Predict\n",
    "# ============================================================\n",
    "def train_model(model, train_loader, val_loader, val_y, device, epochs=EPOCHS, patience=PATIENCE, lr=1e-3):\n",
    "    model.to(device)\n",
    "\n",
    "    pos_ratio = float(np.mean(val_y))\n",
    "    pos_weight = torch.tensor([(1 - pos_ratio) / (pos_ratio + 1e-6)], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_num, X_cat, y in train_loader:\n",
    "            X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_num, X_cat), y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_num, X_cat, _ in val_loader:\n",
    "                X_num, X_cat = X_num.to(device), X_cat.to(device)\n",
    "                val_preds.append(torch.sigmoid(model(X_num, X_cat)).cpu().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds).ravel()\n",
    "        val_auc = roc_auc_score(val_y, val_preds)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-5:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_auc\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X_num, X_cat = batch[0].to(device), batch[1].to(device)\n",
    "            preds.append(torch.sigmoid(model(X_num, X_cat)).cpu().numpy())\n",
    "    return np.concatenate(preds).ravel()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def sanitize(name: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"\", name)\n",
    "\n",
    "def submission_name(exp_name: str, oof_auc: float, seed: int) -> str:\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    exp_name = sanitize(exp_name)\n",
    "    return f\"sub_v5exp-{exp_name}_oof-{oof_auc:.5f}_seed{seed}_{ts}.csv\"\n",
    "\n",
    "def ensure_columns(df, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns in df: {missing[:10]} ... total {len(missing)}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# One experiment runner\n",
    "# ============================================================\n",
    "def run_experiment(exp_name: str, include_groups: list[str], save_submission=True):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üß™ EXP: {exp_name}\")\n",
    "    print(f\"   groups: {include_groups}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    set_seed(SEED)\n",
    "    train_clean = clean_data(train_raw)\n",
    "    test_clean = clean_data(test_raw)\n",
    "\n",
    "    # resolved numeric feature columns (excluding TE since TE is computed per fold)\n",
    "    num_cols = resolve_feature_list([g for g in include_groups if g != \"te\"])\n",
    "    use_te = (\"te\" in include_groups)\n",
    "\n",
    "    # check required raw cols exist post build_features (will after we build)\n",
    "    oof_mlp = np.zeros(len(train_clean))\n",
    "    oof_ft  = np.zeros(len(train_clean))\n",
    "    test_mlp = np.zeros(len(test_clean))\n",
    "    test_ft  = np.zeros(len(test_clean))\n",
    "    fold_auc_mlp, fold_auc_ft = [], []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_clean, train_clean[\"voted_bin\"])):\n",
    "        print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "\n",
    "        tr_df = train_clean.iloc[tr_idx].copy().reset_index(drop=True)\n",
    "        va_df = train_clean.iloc[va_idx].copy().reset_index(drop=True)\n",
    "        te_df = test_clean.copy()\n",
    "\n",
    "        tr_fe = build_features(tr_df)\n",
    "        va_fe = build_features(va_df)\n",
    "        te_fe = build_features(te_df)\n",
    "\n",
    "        # ensure columns exist\n",
    "        ensure_columns(tr_fe, num_cols)\n",
    "        ensure_columns(va_fe, num_cols)\n",
    "        ensure_columns(te_fe, num_cols)\n",
    "\n",
    "        X_tr = tr_fe[num_cols].copy()\n",
    "        X_va = va_fe[num_cols].copy()\n",
    "        X_te = te_fe[num_cols].copy()\n",
    "\n",
    "        # add TE\n",
    "        if use_te:\n",
    "            te_dict = create_target_encodings(tr_fe, va_fe, te_fe, target_col=\"voted_bin\")\n",
    "            for te_name in TE_FEATURES:\n",
    "                X_tr[te_name] = te_dict[\"train\"][te_name]\n",
    "                X_va[te_name] = te_dict[\"val\"][te_name]\n",
    "                X_te[te_name] = te_dict[\"test\"][te_name]\n",
    "\n",
    "        all_num_cols = list(X_tr.columns)\n",
    "\n",
    "        # NaN -> median from train\n",
    "        for c in all_num_cols:\n",
    "            med = X_tr[c].median()\n",
    "            if pd.isna(med):\n",
    "                med = 0\n",
    "            X_tr[c] = X_tr[c].fillna(med)\n",
    "            X_va[c] = X_va[c].fillna(med)\n",
    "            X_te[c] = X_te[c].fillna(med)\n",
    "\n",
    "        # scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_s = scaler.fit_transform(X_tr.values)\n",
    "        X_va_s = scaler.transform(X_va.values)\n",
    "        X_te_s = scaler.transform(X_te.values)\n",
    "\n",
    "        # categorical encoding (always)\n",
    "        cat_dims = []\n",
    "        X_cat_tr, X_cat_va, X_cat_te = [], [], []\n",
    "\n",
    "        for col in CAT_FEATURES:\n",
    "            le = LabelEncoder()\n",
    "            all_vals = list(set(tr_fe[col].fillna(\"NaN\").astype(str)) |\n",
    "                           set(va_fe[col].fillna(\"NaN\").astype(str)) |\n",
    "                           set(te_fe[col].fillna(\"NaN\").astype(str)))\n",
    "            le.fit(all_vals + [\"UNK\"])\n",
    "            cat_dims.append(len(le.classes_))\n",
    "\n",
    "            X_cat_tr.append(le.transform(tr_fe[col].fillna(\"NaN\").astype(str)))\n",
    "            X_cat_va.append(le.transform(va_fe[col].fillna(\"NaN\").astype(str)))\n",
    "            X_cat_te.append(le.transform(te_fe[col].fillna(\"NaN\").astype(str)))\n",
    "\n",
    "        X_cat_tr = np.stack(X_cat_tr, axis=1)\n",
    "        X_cat_va = np.stack(X_cat_va, axis=1)\n",
    "        X_cat_te = np.stack(X_cat_te, axis=1)\n",
    "\n",
    "        y_tr = tr_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "        y_va = va_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "\n",
    "        # loaders\n",
    "        train_ds = TabDataset(X_tr_s, X_cat_tr, y_tr)\n",
    "        val_ds   = TabDataset(X_va_s, X_cat_va, y_va)\n",
    "        test_ds  = TabDataset(X_te_s, X_cat_te)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # MLP\n",
    "        mlp = MLP(num_features=len(all_num_cols), cat_dims=cat_dims, embed_dim=8,\n",
    "                  hidden_dims=[256, 128, 64], dropout=0.3)\n",
    "        mlp, auc_m = train_model(mlp, train_loader, val_loader, y_va, DEVICE, lr=1e-3)\n",
    "        fold_auc_mlp.append(auc_m)\n",
    "\n",
    "        oof_mlp[va_idx] = predict(mlp, val_loader, DEVICE)\n",
    "        test_mlp += predict(mlp, test_loader, DEVICE) / N_FOLDS\n",
    "        print(f\"   MLP fold AUC: {auc_m:.5f}\")\n",
    "\n",
    "        # FT\n",
    "        ft = FTTransformer(num_features=len(all_num_cols), cat_dims=cat_dims, d_token=48, n_layers=2, n_heads=4, dropout=0.2)\n",
    "        ft, auc_f = train_model(ft, train_loader, val_loader, y_va, DEVICE, lr=5e-4)\n",
    "        fold_auc_ft.append(auc_f)\n",
    "\n",
    "        oof_ft[va_idx] = predict(ft, val_loader, DEVICE)\n",
    "        test_ft += predict(ft, test_loader, DEVICE) / N_FOLDS\n",
    "        print(f\"   FT  fold AUC: {auc_f:.5f}\")\n",
    "\n",
    "    # OOF scores\n",
    "    y_true = train_clean[\"voted_bin\"].values\n",
    "    auc_oof_mlp = roc_auc_score(y_true, oof_mlp)\n",
    "    auc_oof_ft  = roc_auc_score(y_true, oof_ft)\n",
    "\n",
    "    oof_avg = (oof_mlp + oof_ft) / 2\n",
    "    auc_oof_avg = roc_auc_score(y_true, oof_avg)\n",
    "\n",
    "    # weight search (coarse)\n",
    "    best_w = 0.5\n",
    "    best_auc = auc_oof_avg\n",
    "    for w in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        auc_w = roc_auc_score(y_true, w * oof_mlp + (1 - w) * oof_ft)\n",
    "        if auc_w > best_auc:\n",
    "            best_auc = auc_w\n",
    "            best_w = w\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"OOF AUC - MLP : {auc_oof_mlp:.5f}  | folds: {[f'{x:.5f}' for x in fold_auc_mlp]}\")\n",
    "    print(f\"OOF AUC - FT  : {auc_oof_ft:.5f}  | folds: {[f'{x:.5f}' for x in fold_auc_ft]}\")\n",
    "    print(f\"OOF AUC - AVG : {auc_oof_avg:.5f}\")\n",
    "    print(f\"OOF AUC - BEST WEIGHT: w={best_w:.1f} (MLP) + {1-best_w:.1f} (FT) => {best_auc:.5f}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # final preds\n",
    "    final_test = best_w * test_mlp + (1 - best_w) * test_ft\n",
    "\n",
    "    if save_submission:\n",
    "        fname = submission_name(exp_name, best_auc, SEED)\n",
    "        outpath = os.path.join(OUTDIR, fname)\n",
    "\n",
    "        sub = pd.DataFrame({\n",
    "            \"index\": test_raw[\"index\"] if \"index\" in test_raw.columns else np.arange(len(test_raw)),\n",
    "            \"voted\": final_test\n",
    "        })\n",
    "        sub.to_csv(outpath, index=False)\n",
    "        print(f\"üíæ saved: {outpath}\")\n",
    "        print(f\"   pred range: [{final_test.min():.4f}, {final_test.max():.4f}]\")\n",
    "\n",
    "    return {\n",
    "        \"exp\": exp_name,\n",
    "        \"oof_mlp\": auc_oof_mlp,\n",
    "        \"oof_ft\": auc_oof_ft,\n",
    "        \"oof_avg\": auc_oof_avg,\n",
    "        \"oof_best\": best_auc,\n",
    "        \"best_w\": best_w\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main: run selected experiments\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    for exp_name, groups in EXPERIMENTS.items():\n",
    "        res = run_experiment(exp_name, groups, save_submission=True)\n",
    "        results.append(res)\n",
    "\n",
    "    # summary\n",
    "    results = sorted(results, key=lambda x: x[\"oof_best\"], reverse=True)\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"üèÅ EXPERIMENT SUMMARY (sorted by oof_best)\")\n",
    "    print(\"=\"*80)\n",
    "    for r in results:\n",
    "        print(f\"{r['exp']:<22} | best={r['oof_best']:.5f} | avg={r['oof_avg']:.5f} | mlp={r['oof_mlp']:.5f} | ft={r['oof_ft']:.5f} | w={r['best_w']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7634b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}