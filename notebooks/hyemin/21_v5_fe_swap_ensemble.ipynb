{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bed6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ Device: cpu\n",
      "\n",
      "ğŸ“‚ ë°ì´í„° ë¡œë”©...\n",
      "Train: (45532, 79), Test: (11383, 77)\n",
      "\n",
      "============================================================\n",
      "ğŸš€ MLP + FT-Transformer ì•™ìƒë¸” í•™ìŠµ (FE êµì²´ ë²„ì „)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ Fold 1/5\n",
      "==================================================\n",
      "\n",
      "  ğŸ”· MLP í•™ìŠµ...\n",
      "    MLP AUC: 0.77493\n",
      "\n",
      "  ğŸ”¶ FT-Transformer í•™ìŠµ...\n",
      "    FT AUC: 0.77680\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ Fold 2/5\n",
      "==================================================\n",
      "\n",
      "  ğŸ”· MLP í•™ìŠµ...\n",
      "    MLP AUC: 0.76664\n",
      "\n",
      "  ğŸ”¶ FT-Transformer í•™ìŠµ...\n",
      "    FT AUC: 0.76755\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ Fold 3/5\n",
      "==================================================\n",
      "\n",
      "  ğŸ”· MLP í•™ìŠµ...\n",
      "    MLP AUC: 0.76235\n",
      "\n",
      "  ğŸ”¶ FT-Transformer í•™ìŠµ...\n",
      "    FT AUC: 0.75922\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ Fold 4/5\n",
      "==================================================\n",
      "\n",
      "  ğŸ”· MLP í•™ìŠµ...\n",
      "    MLP AUC: 0.76007\n",
      "\n",
      "  ğŸ”¶ FT-Transformer í•™ìŠµ...\n",
      "    FT AUC: 0.75820\n",
      "\n",
      "==================================================\n",
      "ğŸ“‚ Fold 5/5\n",
      "==================================================\n",
      "\n",
      "  ğŸ”· MLP í•™ìŠµ...\n",
      "    MLP AUC: 0.76783\n",
      "\n",
      "  ğŸ”¶ FT-Transformer í•™ìŠµ...\n",
      "    FT AUC: 0.76889\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ ì•™ìƒë¸” ê²°ê³¼\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š MLP:\n",
      "   OOF AUC: 0.76620\n",
      "   Fold AUCs: ['0.77493', '0.76664', '0.76235', '0.76007', '0.76783']\n",
      "\n",
      "ğŸ“Š FT-Transformer:\n",
      "   OOF AUC: 0.76559\n",
      "   Fold AUCs: ['0.77680', '0.76755', '0.75922', '0.75820', '0.76889']\n",
      "\n",
      "ğŸ† ì•™ìƒë¸” (MLP + FT í‰ê· ):\n",
      "   OOF AUC: 0.76917\n",
      "\n",
      "ğŸ† ìµœì  ê°€ì¤‘ ì•™ìƒë¸” (MLP:0.5 + FT:0.5):\n",
      "   OOF AUC: 0.76917\n",
      "\n",
      "ğŸ’¾ ì €ì¥: submission_21_v5_fe_swap_ensemble.csv\n",
      "   ì˜ˆì¸¡ ë²”ìœ„: [0.1206, 0.9940]\n",
      "   (ê°œë³„ ì €ì¥: submission_21_v5_fe_swap_mlp.csv, submission_21_v5_fe_swap_ft.csv)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ¯ íˆ¬í‘œ ì˜ˆì¸¡ ëª¨ë¸ v5 (FE êµì²´ ë²„ì „) - MLP + FT-Transformer ì•™ìƒë¸”\n",
    "===================================================\n",
    "- ê¸°ì¡´ v5 í•™ìŠµ/ëª¨ë¸ êµ¬ì¡° ìœ ì§€\n",
    "- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ë§Œ (clean_data + build_features(train_df, test_df))ë¡œ êµì²´\n",
    "- Foldë³„ë¡œ train ê¸°ì¤€ ì¸ì½”ë”© ìœ ì§€ (train-val / train-test ê°ê° ìƒì„±)\n",
    "- Cat embeddingì€ LabelEncoded(age_group, gender, race, religion) ì‚¬ìš©\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "PATIENCE = 8\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ë„ˆê°€ í•­ìƒ ì“°ëŠ” ê¸°ë³¸ ê²½ë¡œë¡œ ë§ì¶¤\n",
    "TRAIN_PATH = \"../../data/raw/train.csv\"\n",
    "TEST_PATH  = \"../../data/raw/test_x.csv\"\n",
    "\n",
    "print(f\"ğŸ–¥ï¸ Device: {DEVICE}\")\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "# ============================================================\n",
    "print(\"\\nğŸ“‚ ë°ì´í„° ë¡œë”©...\")\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_raw[\"voted_bin\"] = (train_raw[\"voted\"] == 2).astype(int)\n",
    "print(f\"Train: {train_raw.shape}, Test: {test_raw.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# ì´ìƒì¹˜/ë¬´ì‘ë‹µ ì²˜ë¦¬ (ì‚¬ìš©ì ì œê³µ ë²„ì „ ê·¸ëŒ€ë¡œ)\n",
    "# ============================================================\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ë¬´ì‘ë‹µ (0 â†’ NaN -> Mean)\n",
    "    for col in [\"education\", \"engnat\", \"hand\", \"married\", \"urban\"]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    # familysize\n",
    "    if \"familysize\" in df.columns:\n",
    "        df.loc[df[\"familysize\"] == 0, \"familysize\"] = np.nan\n",
    "        df.loc[df[\"familysize\"] > 15, \"familysize\"] = np.nan\n",
    "        df[\"familysize\"] = df[\"familysize\"].fillna(df[\"familysize\"].mean())\n",
    "\n",
    "    # TP 0 â†’ NaN -> Mean\n",
    "    for col in [f\"tp{i:02d}\" for i in range(1, 11)]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    # Q_E í´ë¦¬í•‘\n",
    "    for col in [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(lower=100, upper=60000)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì‚¬ìš©ì ì œê³µ ë²„ì „ + foldë³„ train ê¸°ì¤€ ì¸ì½”ë”©)\n",
    "# ============================================================\n",
    "def build_features(train_df, other_df):\n",
    "    \"\"\"\n",
    "    - train_df ê¸°ì¤€ìœ¼ë¡œ LabelEncoding fit\n",
    "    - other_dfëŠ” ê°™ì€ encoderë¡œ transform\n",
    "    - ë‘ DF ëª¨ë‘ ë™ì¼í•œ íŒŒìƒë³€ìˆ˜ ìƒì„±/ì›ë³¸ drop\n",
    "    \"\"\"\n",
    "    train = train_df.copy()\n",
    "    other = other_df.copy()\n",
    "    dataset = [train, other]\n",
    "\n",
    "    # Q_A\n",
    "    qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "\n",
    "    for data in dataset:\n",
    "        # (ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì•ˆì „ ì²˜ë¦¬)\n",
    "        def safe_get(col):\n",
    "            return data[col] if col in data.columns else 0\n",
    "\n",
    "        data[\"T\"] = safe_get(\"QcA\") - safe_get(\"QfA\") + safe_get(\"QoA\") - safe_get(\"QrA\") + safe_get(\"QsA\")\n",
    "        data[\"V\"] = safe_get(\"QbA\") - safe_get(\"QeA\") + safe_get(\"QhA\") + safe_get(\"QjA\") + safe_get(\"QmA\") - safe_get(\"QqA\")\n",
    "        data[\"M\"] = -safe_get(\"QkA\")\n",
    "\n",
    "    # ì—­ì±„ì  ì§ˆë¬¸ë“¤\n",
    "    flipping_columns = [\"QeA\", \"QfA\", \"QkA\", \"QqA\", \"QrA\"]\n",
    "    for data in dataset:\n",
    "        for flip in flipping_columns:\n",
    "            if flip in data.columns:\n",
    "                data[flip] = 6 - data[flip]\n",
    "\n",
    "    # Secret ì§ˆë¬¸ë“¤\n",
    "    flipping_secret_columns = [\"QaA\", \"QdA\", \"QgA\", \"QiA\", \"QnA\"]\n",
    "    for data in dataset:\n",
    "        for flip in flipping_secret_columns:\n",
    "            if flip in data.columns:\n",
    "                data[flip] = 6 - data[flip]\n",
    "\n",
    "    for data in dataset:\n",
    "        # qa_colsê°€ ë‹¤ ìˆì§€ ì•Šìœ¼ë©´ ìˆëŠ” ê²ƒë§Œ í‰ê· \n",
    "        existing_qa = [c for c in qa_cols if c in data.columns]\n",
    "        if len(existing_qa) > 0:\n",
    "            data[\"Mach_score\"] = data[existing_qa].mean(axis=1)\n",
    "        else:\n",
    "            data[\"Mach_score\"] = 0.0\n",
    "\n",
    "    # Q_E -> delay\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    for data in dataset:\n",
    "        existing_qe = [c for c in qe_cols if c in data.columns]\n",
    "        if len(existing_qe) > 0:\n",
    "            data[\"delay\"] = data[existing_qe].sum(axis=1)\n",
    "            data[\"delay\"] = data[\"delay\"] ** (1 / 10)\n",
    "        else:\n",
    "            data[\"delay\"] = 0.0\n",
    "\n",
    "    # ì›ë³¸ Q_A, Q_E ì œê±°\n",
    "    for data in dataset:\n",
    "        drop_qa = [c for c in qa_cols if c in data.columns]\n",
    "        drop_qe = [c for c in qe_cols if c in data.columns]\n",
    "        if len(drop_qa) > 0:\n",
    "            data.drop(drop_qa, axis=1, inplace=True)\n",
    "        if len(drop_qe) > 0:\n",
    "            data.drop(drop_qe, axis=1, inplace=True)\n",
    "\n",
    "    # TP Big5\n",
    "    for data in dataset:\n",
    "        # ì•ˆì „ ê³„ì‚° (ì—†ëŠ” colì€ 0 ì²˜ë¦¬)\n",
    "        def col_or0(c): return data[c] if c in data.columns else 0\n",
    "        data[\"extraversion\"] = col_or0(\"tp01\") - col_or0(\"tp06\")\n",
    "        data[\"agreeableness\"] = col_or0(\"tp07\") - col_or0(\"tp02\")\n",
    "        data[\"conscientiousness\"] = col_or0(\"tp03\") - col_or0(\"tp08\")\n",
    "        data[\"neuroticism\"] = col_or0(\"tp04\") - col_or0(\"tp09\")\n",
    "        data[\"openness\"] = col_or0(\"tp05\") - col_or0(\"tp10\")\n",
    "\n",
    "    # 10ëŒ€ ì—¬ë¶€\n",
    "    for data in dataset:\n",
    "        data[\"teenager_ox\"] = (data[\"age_group\"].astype(str) == \"10s\").astype(int)\n",
    "\n",
    "    # Label Encoding: train ê¸°ì¤€ fit -> other transform\n",
    "    needenco = [\"age_group\", \"gender\", \"race\", \"religion\"]\n",
    "    for col in needenco:\n",
    "        le = LabelEncoder()\n",
    "        train_col = train[col].astype(str).fillna(\"NaN\") if col in train.columns else pd.Series([\"NaN\"] * len(train))\n",
    "        other_col = other[col].astype(str).fillna(\"NaN\") if col in other.columns else pd.Series([\"NaN\"] * len(other))\n",
    "\n",
    "        # otherì— trainì— ì—†ëŠ” ê°’ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ unionìœ¼ë¡œ fit\n",
    "        all_vals = pd.concat([train_col, other_col], axis=0).astype(str).values\n",
    "        le.fit(all_vals)\n",
    "\n",
    "        train[col] = le.transform(train_col)\n",
    "        other[col] = le.transform(other_col)\n",
    "\n",
    "    # ì„±ë³„ ìƒí˜¸ì‘ìš©\n",
    "    for data in dataset:\n",
    "        data[\"Es_gender\"] = data[\"neuroticism\"] * data[\"gender\"]\n",
    "        data[\"Con_gender\"] = data[\"conscientiousness\"] * data[\"gender\"]\n",
    "        data[\"Op_gender\"] = data[\"openness\"] * data[\"gender\"]\n",
    "\n",
    "    # drop list\n",
    "    drop_list = [\"index\", \"hand\"]\n",
    "    for data in dataset:\n",
    "        for c in drop_list:\n",
    "            if c in data.columns:\n",
    "                data.drop(c, axis=1, inplace=True)\n",
    "\n",
    "    # WR ì¼ë¶€ë§Œ ìœ ì§€\n",
    "    wr_list = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    keep_wr = [\"wr_01\", \"wr_03\", \"wr_06\", \"wr_09\", \"wr_11\"]\n",
    "    wr_no_need = [c for c in wr_list if c not in keep_wr]\n",
    "    for data in dataset:\n",
    "        for c in wr_no_need:\n",
    "            if c in data.columns:\n",
    "                data.drop(c, axis=1, inplace=True)\n",
    "\n",
    "    return train, other\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "# ============================================================\n",
    "# Model 1: MLP\n",
    "# ============================================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, embed_dim=8, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.cat_dims = cat_dims\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim + 1, embed_dim) for dim in cat_dims])\n",
    "\n",
    "        input_dim = num_features + len(cat_dims) * embed_dim\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_dims[-1], 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        if x_cat.shape[1] == 0:\n",
    "            x = x_num\n",
    "        else:\n",
    "            cat_embeds = torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=1)\n",
    "            x = torch.cat([x_num, cat_embeds], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# ============================================================\n",
    "# Model 2: FT-Transformer (ê²½ëŸ‰í™”)\n",
    "# ============================================================\n",
    "class NumericalEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, d_token) * 0.02)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features, d_token))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, d_token=48, n_layers=2, n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_embed = NumericalEmbedding(num_features, d_token)\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(dim + 1, d_token) for dim in cat_dims])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token, nhead=n_heads, dim_feedforward=d_token * 2,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        num_tokens = self.num_embed(x_num)  # (B, num_f, d)\n",
    "\n",
    "        if x_cat.shape[1] == 0:\n",
    "            tokens = num_tokens\n",
    "        else:\n",
    "            cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)], dim=1)  # (B, cat_f, d)\n",
    "            tokens = torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "\n",
    "        cls = self.cls_token.expand(tokens.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        x = self.transformer(tokens)\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "# ============================================================\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "# ============================================================\n",
    "def train_model(model, train_loader, val_loader, val_y, device, epochs=EPOCHS, patience=PATIENCE, lr=1e-3):\n",
    "    model.to(device)\n",
    "\n",
    "    pos_ratio = float(np.mean(val_y))\n",
    "    pos_weight = torch.tensor([(1 - pos_ratio) / (pos_ratio + 1e-6)], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_num, X_cat, y in train_loader:\n",
    "            X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_num, X_cat), y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_num, X_cat, _ in val_loader:\n",
    "                X_num, X_cat = X_num.to(device), X_cat.to(device)\n",
    "                val_preds.append(torch.sigmoid(model(X_num, X_cat)).cpu().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds).ravel()\n",
    "        val_auc = roc_auc_score(val_y, val_preds)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-5:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_auc\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            X_num, X_cat = batch[0].to(device), batch[1].to(device)\n",
    "            preds.append(torch.sigmoid(model(X_num, X_cat)).cpu().numpy())\n",
    "    return np.concatenate(preds).ravel()\n",
    "\n",
    "# ============================================================\n",
    "# ë©”ì¸\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ MLP + FT-Transformer ì•™ìƒë¸” í•™ìŠµ (FE êµì²´ ë²„ì „)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    set_seed(SEED)\n",
    "\n",
    "    # cleanì€ ì „ì²´ì— 1ë²ˆ ì ìš©í•´ë„ ë˜ì§€ë§Œ,\n",
    "    # foldë³„ë¡œ train/val/test ë¶„ë¦¬ í›„ì—ë„ ë™ì¼í•˜ê²Œ ì ìš©ë˜ë„ë¡ fold ë‚´ë¶€ì—ì„œ í•œ ë²ˆ ë” ì ìš©í•¨\n",
    "    train_base = train_raw.copy()\n",
    "    test_base  = test_raw.copy()\n",
    "\n",
    "    oof_mlp = np.zeros(len(train_base))\n",
    "    oof_ft  = np.zeros(len(train_base))\n",
    "    test_mlp = np.zeros(len(test_base))\n",
    "    test_ft  = np.zeros(len(test_base))\n",
    "\n",
    "    fold_aucs_mlp, fold_aucs_ft = [], []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_base, train_base[\"voted_bin\"])):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ğŸ“‚ Fold {fold+1}/{N_FOLDS}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        train_fold = train_base.iloc[tr_idx].copy().reset_index(drop=True)\n",
    "        val_fold   = train_base.iloc[va_idx].copy().reset_index(drop=True)\n",
    "        test_fold  = test_base.copy().reset_index(drop=True)\n",
    "\n",
    "        # clean\n",
    "        train_fold = clean_data(train_fold)\n",
    "        val_fold   = clean_data(val_fold)\n",
    "        test_fold  = clean_data(test_fold)\n",
    "\n",
    "        # FE (train ê¸°ì¤€)\n",
    "        train_fe, val_fe = build_features(train_fold, val_fold)\n",
    "        _, test_fe       = build_features(train_fold, test_fold)\n",
    "\n",
    "        # voted_bin ë³´ì¡´\n",
    "        train_fe[\"voted_bin\"] = train_fold[\"voted_bin\"].values\n",
    "        val_fe[\"voted_bin\"]   = val_fold[\"voted_bin\"].values\n",
    "\n",
    "        # cat / num ì •ì˜\n",
    "        cat_features = [\"age_group\", \"gender\", \"race\", \"religion\"]\n",
    "        cat_features = [c for c in cat_features if c in train_fe.columns]  # í˜¹ì‹œ ì—†ìœ¼ë©´ ì œì™¸\n",
    "        num_features = [c for c in train_fe.columns if c not in ([\"voted\", \"voted_bin\"] + cat_features)]\n",
    "\n",
    "        # numeric matrix\n",
    "        X_train = train_fe[num_features].copy()\n",
    "        X_val   = val_fe[num_features].copy()\n",
    "        X_test  = test_fe[num_features].copy()\n",
    "\n",
    "        # NaN -> median\n",
    "        for col in num_features:\n",
    "            med = X_train[col].median()\n",
    "            if pd.isna(med):\n",
    "                med = 0.0\n",
    "            X_train[col] = X_train[col].fillna(med)\n",
    "            X_val[col]   = X_val[col].fillna(med)\n",
    "            X_test[col]  = X_test[col].fillna(med)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train.values)\n",
    "        X_val_s   = scaler.transform(X_val.values)\n",
    "        X_test_s  = scaler.transform(X_test.values)\n",
    "\n",
    "        # cat matrix\n",
    "        if len(cat_features) == 0:\n",
    "            X_cat_train = np.zeros((len(train_fe), 0), dtype=np.int64)\n",
    "            X_cat_val   = np.zeros((len(val_fe), 0), dtype=np.int64)\n",
    "            X_cat_test  = np.zeros((len(test_fe), 0), dtype=np.int64)\n",
    "            cat_dims = []\n",
    "        else:\n",
    "            X_cat_train = train_fe[cat_features].fillna(0).astype(int).values\n",
    "            X_cat_val   = val_fe[cat_features].fillna(0).astype(int).values\n",
    "            X_cat_test  = test_fe[cat_features].fillna(0).astype(int).values\n",
    "\n",
    "            # dim = max+1 (train/val/test í†µí•© ê¸°ì¤€)\n",
    "            cat_dims = []\n",
    "            for j in range(X_cat_train.shape[1]):\n",
    "                m = int(max(X_cat_train[:, j].max(), X_cat_val[:, j].max(), X_cat_test[:, j].max()))\n",
    "                cat_dims.append(m + 1)\n",
    "\n",
    "        y_train = train_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "        y_val   = val_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "\n",
    "        # DataLoader\n",
    "        train_ds = TabDataset(X_train_s, X_cat_train, y_train)\n",
    "        val_ds   = TabDataset(X_val_s, X_cat_val, y_val)\n",
    "        test_ds  = TabDataset(X_test_s, X_cat_test)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # =====================\n",
    "        # Model 1: MLP\n",
    "        # =====================\n",
    "        print(\"\\n  ğŸ”· MLP í•™ìŠµ...\")\n",
    "        mlp = MLP(\n",
    "            num_features=X_train_s.shape[1],\n",
    "            cat_dims=cat_dims,\n",
    "            embed_dim=8,\n",
    "            hidden_dims=[256, 128, 64],\n",
    "            dropout=0.3\n",
    "        )\n",
    "        mlp, auc_mlp = train_model(mlp, train_loader, val_loader, y_val, DEVICE, lr=1e-3)\n",
    "        fold_aucs_mlp.append(auc_mlp)\n",
    "        print(f\"    MLP AUC: {auc_mlp:.5f}\")\n",
    "\n",
    "        oof_mlp[va_idx] = predict(mlp, val_loader, DEVICE)\n",
    "        test_mlp += predict(mlp, test_loader, DEVICE) / N_FOLDS\n",
    "\n",
    "        # =====================\n",
    "        # Model 2: FT-Transformer\n",
    "        # =====================\n",
    "        print(\"\\n  ğŸ”¶ FT-Transformer í•™ìŠµ...\")\n",
    "        ft = FTTransformer(\n",
    "            num_features=X_train_s.shape[1],\n",
    "            cat_dims=cat_dims,\n",
    "            d_token=48,\n",
    "            n_layers=2,\n",
    "            n_heads=4,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        ft, auc_ft = train_model(ft, train_loader, val_loader, y_val, DEVICE, lr=5e-4)\n",
    "        fold_aucs_ft.append(auc_ft)\n",
    "        print(f\"    FT AUC: {auc_ft:.5f}\")\n",
    "\n",
    "        oof_ft[va_idx] = predict(ft, val_loader, DEVICE)\n",
    "        test_ft += predict(ft, test_loader, DEVICE) / N_FOLDS\n",
    "\n",
    "    # =====================\n",
    "    # ì•™ìƒë¸”\n",
    "    # =====================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ ì•™ìƒë¸” ê²°ê³¼\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    oof_auc_mlp = roc_auc_score(train_base[\"voted_bin\"], oof_mlp)\n",
    "    oof_auc_ft  = roc_auc_score(train_base[\"voted_bin\"], oof_ft)\n",
    "\n",
    "    print(f\"\\nğŸ“Š MLP:\")\n",
    "    print(f\"   OOF AUC: {oof_auc_mlp:.5f}\")\n",
    "    print(f\"   Fold AUCs: {[f'{x:.5f}' for x in fold_aucs_mlp]}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š FT-Transformer:\")\n",
    "    print(f\"   OOF AUC: {oof_auc_ft:.5f}\")\n",
    "    print(f\"   Fold AUCs: {[f'{x:.5f}' for x in fold_aucs_ft]}\")\n",
    "\n",
    "    oof_ensemble = (oof_mlp + oof_ft) / 2\n",
    "    test_ensemble = (test_mlp + test_ft) / 2\n",
    "    oof_auc_ensemble = roc_auc_score(train_base[\"voted_bin\"], oof_ensemble)\n",
    "\n",
    "    print(f\"\\nğŸ† ì•™ìƒë¸” (MLP + FT í‰ê· ):\")\n",
    "    print(f\"   OOF AUC: {oof_auc_ensemble:.5f}\")\n",
    "\n",
    "    best_w = 0.5\n",
    "    best_auc = oof_auc_ensemble\n",
    "    for w in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        oof_w = w * oof_mlp + (1 - w) * oof_ft\n",
    "        auc_w = roc_auc_score(train_base[\"voted_bin\"], oof_w)\n",
    "        if auc_w > best_auc:\n",
    "            best_auc = auc_w\n",
    "            best_w = w\n",
    "\n",
    "    print(f\"\\nğŸ† ìµœì  ê°€ì¤‘ ì•™ìƒë¸” (MLP:{best_w:.1f} + FT:{1-best_w:.1f}):\")\n",
    "    print(f\"   OOF AUC: {best_auc:.5f}\")\n",
    "\n",
    "    final_test = best_w * test_mlp + (1 - best_w) * test_ft\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        \"index\": test_raw[\"index\"] if \"index\" in test_raw.columns else range(len(test_raw)),\n",
    "        \"voted\": final_test\n",
    "    })\n",
    "    sub.to_csv(\"submission_21_v5_fe_swap_ensemble.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nğŸ’¾ ì €ì¥: submission_21_v5_fe_swap_ensemble.csv\")\n",
    "    print(f\"   ì˜ˆì¸¡ ë²”ìœ„: [{final_test.min():.4f}, {final_test.max():.4f}]\")\n",
    "\n",
    "    # ê°œë³„ ì €ì¥\n",
    "    pd.DataFrame({\"index\": sub[\"index\"], \"voted\": test_mlp}).to_csv(\"submission_21_v5_fe_swap_mlp.csv\", index=False)\n",
    "    pd.DataFrame({\"index\": sub[\"index\"], \"voted\": test_ft}).to_csv(\"submission_21_v5_fe_swap_ft.csv\", index=False)\n",
    "    print(\"   (ê°œë³„ ì €ì¥: submission_21_v5_fe_swap_mlp.csv, submission_21_v5_fe_swap_ft.csv)\")\n",
    "\n",
    "    return best_auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}