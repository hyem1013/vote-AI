{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8387cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cpu\n",
      "‚úÖ Loaded best params: {'d_token': 64, 'n_layers': 3, 'n_heads': 8, 'dropout': 0.09030532181350111, 'lr': 0.0017502789790582254, 'weight_decay': 0.00026568139241144923, 'warmup_ratio': 0.10600845078125082, 'batch_size': 512}\n",
      "\n",
      "================================================================================\n",
      "üå± FULL TRAIN seed=42\n",
      "================================================================================\n",
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "seed=42 | best_auc=0.77133 | best_w=0.64\n",
      "\n",
      "================================================================================\n",
      "üå± FULL TRAIN seed=202\n",
      "================================================================================\n",
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "seed=202 | best_auc=0.77255 | best_w=0.57\n",
      "\n",
      "================================================================================\n",
      "üå± FULL TRAIN seed=777\n",
      "================================================================================\n",
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "seed=777 | best_auc=0.77202 | best_w=0.54\n",
      "\n",
      "================================================================================\n",
      "üå± FULL TRAIN seed=1024\n",
      "================================================================================\n",
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "seed=1024 | best_auc=0.77196 | best_w=0.55\n",
      "\n",
      "================================================================================\n",
      "üå± FULL TRAIN seed=2048\n",
      "================================================================================\n",
      "\n",
      "[Fold 1/5]\n",
      "\n",
      "[Fold 2/5]\n",
      "\n",
      "[Fold 3/5]\n",
      "\n",
      "[Fold 4/5]\n",
      "\n",
      "[Fold 5/5]\n",
      "seed=2048 | best_auc=0.77227 | best_w=0.59\n",
      "\n",
      "################################################################################\n",
      "üèÅ FINAL OOF AUC: 0.77427\n",
      "################################################################################\n",
      "üíæ Saved to: ../../data/raw/outputs\n",
      " - submission_colab_v7_optuna_ensemble.csv\n",
      " - submission_colab_v7_optuna_mlp.csv\n",
      " - submission_colab_v7_optuna_ft.csv\n",
      " - report_colab_v7_optuna.json\n"
     ]
    }
   ],
   "source": [
    "# train_vote_v7_optuna.py\n",
    "# ============================================================\n",
    "# Vote Prediction v7 - Optuna tuning (FT) -> Full Seed Ensemble\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Paths / Config\n",
    "# -------------------------\n",
    "BASE_DIR = \"../../data/raw\"\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = os.path.join(BASE_DIR, \"train.csv\")\n",
    "TEST_PATH  = os.path.join(BASE_DIR, \"test_x.csv\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_AMP = True\n",
    "\n",
    "# Full training seeds (after tuning)\n",
    "SEEDS_FULL = [42, 202, 777, 1024, 2048]\n",
    "N_FOLDS_FULL = 5\n",
    "\n",
    "# Tuning setting (cheap & fast)\n",
    "TUNE_SEED = 42\n",
    "TUNE_N_FOLDS = 1           # ‚úÖ ÌäúÎãùÏùÄ 1 foldÎ°úÎßå\n",
    "TUNE_MAX_EPOCHS = 60       # ‚úÖ ÌäúÎãùÏùÄ ÏßßÍ≤å\n",
    "TUNE_PATIENCE = 8\n",
    "\n",
    "# Train setting (final)\n",
    "EPOCHS = 120\n",
    "PATIENCE = 12\n",
    "\n",
    "# Fixed MLP config (ÌäúÎãùÏùÄ FTÎßå)\n",
    "MLP_CFG = dict(\n",
    "    embed_dim=8,\n",
    "    hidden_dims=[384, 192, 96],\n",
    "    dropout=0.30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "W_STEP = 0.01  # ensemble weight search step\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Clean / Feature Eng\n",
    "# -------------------------\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in [\"education\", \"engnat\", \"hand\", \"married\", \"urban\"]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    if \"familysize\" in df.columns:\n",
    "        df.loc[df[\"familysize\"] == 0, \"familysize\"] = np.nan\n",
    "        df.loc[df[\"familysize\"] > 15, \"familysize\"] = np.nan\n",
    "\n",
    "    for col in [f\"tp{i:02d}\" for i in range(1, 11)]:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    for col in [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(lower=100, upper=60000)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    age_map = {\"10s\": 1, \"20s\": 2, \"30s\": 3, \"40s\": 4, \"50s\": 5, \"60s\": 6, \"+70s\": 7}\n",
    "    df[\"age_ord\"] = df[\"age_group\"].map(age_map)\n",
    "\n",
    "    df[\"is_teenager\"] = (df[\"age_ord\"] == 1).astype(int)\n",
    "    df[\"is_young\"]    = (df[\"age_ord\"] <= 2).astype(int)\n",
    "    df[\"is_old\"]      = (df[\"age_ord\"] >= 6).astype(int)\n",
    "\n",
    "    df[\"edu_low\"]  = (df[\"education\"] <= 2).astype(float)\n",
    "    df[\"edu_high\"] = (df[\"education\"] >= 3).astype(float)\n",
    "\n",
    "    df[\"is_single\"]  = (df[\"married\"] == 1).astype(float)\n",
    "    df[\"is_married\"] = (df[\"married\"] == 2).astype(float)\n",
    "\n",
    "    df[\"is_urban\"]          = (df[\"urban\"] == 3).astype(float)\n",
    "    df[\"is_english_native\"] = (df[\"engnat\"] == 1).astype(float)\n",
    "    df[\"is_male\"]           = (df[\"gender\"] == \"Male\").astype(int)\n",
    "\n",
    "    qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    df[\"qa_mean\"] = df[qa_cols].mean(axis=1)\n",
    "    df[\"qa_std\"]  = df[qa_cols].std(axis=1)\n",
    "    df[\"qa_range\"] = df[qa_cols].max(axis=1) - df[qa_cols].min(axis=1)\n",
    "    df[\"qa_extreme_ratio\"] = ((df[qa_cols] == 1) | (df[qa_cols] == 5)).sum(axis=1) / 20\n",
    "    df[\"qa_neutral_ratio\"] = (df[qa_cols] == 3).sum(axis=1) / 20\n",
    "    df[\"qa_all_same\"] = (df[qa_cols].std(axis=1) == 0).astype(int)\n",
    "\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    for col in qe_cols:\n",
    "        df[f\"{col}_log\"] = np.log1p(df[col])\n",
    "\n",
    "    qe_log_cols = [f\"{col}_log\" for col in qe_cols]\n",
    "    df[\"qe_log_mean\"] = df[qe_log_cols].mean(axis=1)\n",
    "    df[\"qe_log_std\"]  = df[qe_log_cols].std(axis=1)\n",
    "    df[\"qe_fast_ratio\"] = (df[qe_cols] < 500).sum(axis=1) / 20\n",
    "    df[\"qe_total_log\"]  = df[qe_log_cols].sum(axis=1)\n",
    "    df[\"is_careless\"] = ((df[qe_cols].mean(axis=1) < 500) | (df[\"qa_all_same\"] == 1)).astype(int)\n",
    "\n",
    "    tp_cols = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "    df[\"tp_missing_ratio\"] = df[tp_cols].isna().sum(axis=1) / 10\n",
    "    df[\"extraversion\"]      = df[\"tp01\"] - df[\"tp06\"]\n",
    "    df[\"agreeableness\"]     = df[\"tp07\"] - df[\"tp02\"]\n",
    "    df[\"conscientiousness\"] = df[\"tp03\"] - df[\"tp08\"]\n",
    "    df[\"neuroticism\"]       = df[\"tp04\"] - df[\"tp09\"]\n",
    "    df[\"openness\"]          = df[\"tp05\"] - df[\"tp10\"]\n",
    "    df[\"tp_mean\"] = df[tp_cols].mean(axis=1)\n",
    "\n",
    "    wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "    df[\"wr_sum\"] = df[wr_cols].sum(axis=1)\n",
    "    df[\"wf_sum\"] = df[wf_cols].sum(axis=1)\n",
    "    df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
    "    df[\"vocab_high\"] = (df[\"wr_sum\"] >= 11).astype(int)\n",
    "\n",
    "    df[\"age_edu\"] = df[\"age_ord\"] * df[\"education\"]\n",
    "    df[\"young_low_edu\"] = df[\"is_young\"] * df[\"edu_low\"]\n",
    "    df[\"young_single\"]  = df[\"is_young\"] * df[\"is_single\"]\n",
    "    df[\"old_married\"]   = df[\"is_old\"] * df[\"is_married\"]\n",
    "    df[\"teenager_low_edu\"] = df[\"is_teenager\"] * df[\"edu_low\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def target_encode(train_df, val_df, test_df, col, target_col, smoothing=10):\n",
    "    global_mean = train_df[target_col].mean()\n",
    "    agg = train_df.groupby(col)[target_col].agg([\"mean\", \"count\"])\n",
    "    agg[\"te\"] = (agg[\"count\"] * agg[\"mean\"] + smoothing * global_mean) / (agg[\"count\"] + smoothing)\n",
    "    te_map = agg[\"te\"].to_dict()\n",
    "    return (\n",
    "        train_df[col].map(te_map).fillna(global_mean).values,\n",
    "        val_df[col].map(te_map).fillna(global_mean).values,\n",
    "        test_df[col].map(te_map).fillna(global_mean).values,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_target_encodings(train_df, val_df, test_df, target_col=\"voted_bin\"):\n",
    "    te_dict = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "    for col in [\"age_group\", \"race\", \"religion\"]:\n",
    "        tr, va, te = target_encode(train_df, val_df, test_df, col, target_col, smoothing=10)\n",
    "        te_dict[\"train\"][f\"{col}_te\"] = tr\n",
    "        te_dict[\"val\"][f\"{col}_te\"]   = va\n",
    "        te_dict[\"test\"][f\"{col}_te\"]  = te\n",
    "\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[\"age_edu_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"education\"].astype(str)\n",
    "        df[\"age_married_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"married\"].astype(str)\n",
    "        df[\"age_race_cat\"] = df[\"age_group\"].astype(str) + \"_\" + df[\"race\"].astype(str)\n",
    "        df[\"age_edu_married_cat\"] = (\n",
    "            df[\"age_group\"].astype(str) + \"_\" + df[\"education\"].astype(str) + \"_\" + df[\"married\"].astype(str)\n",
    "        )\n",
    "\n",
    "    for col, sm in [(\"age_edu_cat\", 5), (\"age_married_cat\", 5), (\"age_race_cat\", 5), (\"age_edu_married_cat\", 3)]:\n",
    "        tr, va, te = target_encode(train_df, val_df, test_df, col, target_col, smoothing=sm)\n",
    "        te_dict[\"train\"][f\"{col}_te\"] = tr\n",
    "        te_dict[\"val\"][f\"{col}_te\"]   = va\n",
    "        te_dict[\"test\"][f\"{col}_te\"]  = te\n",
    "\n",
    "    return te_dict\n",
    "\n",
    "\n",
    "qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "qe_log_cols = [f\"Q{c}E_log\" for c in \"abcdefghijklmnopqrst\"]\n",
    "wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "tp_cols = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "\n",
    "num_features = (\n",
    "    qa_cols + qe_log_cols + wr_cols + wf_cols + tp_cols\n",
    "    + [\n",
    "        \"age_ord\", \"education\", \"married\", \"urban\", \"engnat\", \"familysize\", \"hand\",\n",
    "        \"is_teenager\", \"is_young\", \"is_old\", \"edu_low\", \"edu_high\",\n",
    "        \"is_single\", \"is_married\", \"is_urban\", \"is_english_native\", \"is_male\",\n",
    "        \"qa_mean\", \"qa_std\", \"qa_range\", \"qa_extreme_ratio\", \"qa_neutral_ratio\", \"qa_all_same\",\n",
    "        \"qe_log_mean\", \"qe_log_std\", \"qe_fast_ratio\", \"qe_total_log\", \"is_careless\",\n",
    "        \"tp_missing_ratio\", \"tp_mean\",\n",
    "        \"extraversion\", \"agreeableness\", \"conscientiousness\", \"neuroticism\", \"openness\",\n",
    "        \"wr_sum\", \"wf_sum\", \"word_credibility\", \"vocab_high\",\n",
    "        \"age_edu\", \"young_low_edu\", \"young_single\", \"old_married\", \"teenager_low_edu\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "te_features = [\n",
    "    \"age_group_te\", \"race_te\", \"religion_te\",\n",
    "    \"age_edu_cat_te\", \"age_married_cat_te\", \"age_race_cat_te\", \"age_edu_married_cat_te\",\n",
    "]\n",
    "\n",
    "cat_features = [\"gender\", \"race\", \"religion\"]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Models\n",
    "# -------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, embed_dim=8, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim + 1, embed_dim) for dim in cat_dims])\n",
    "\n",
    "        input_dim = num_features + len(cat_dims) * embed_dim\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = h\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dims[-1], 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        cat_emb = torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)], dim=1)\n",
    "        x = torch.cat([x_num, cat_emb], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class NumericalEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, d_token) * 0.02)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features, d_token))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, d_token=128, n_layers=4, n_heads=8, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.num_embed = NumericalEmbedding(num_features, d_token)\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(dim + 1, d_token) for dim in cat_dims])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token) * 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_token * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        num_tokens = self.num_embed(x_num)\n",
    "        cat_tokens = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)], dim=1)\n",
    "\n",
    "        tokens = torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "        cls = self.cls_token.expand(tokens.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        x = self.transformer(tokens)\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Scheduler\n",
    "# -------------------------\n",
    "def make_cosine_warmup_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.05):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / max(1, warmup_steps)\n",
    "        progress = float(step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        cosine = 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "        return min_lr_ratio + (1.0 - min_lr_ratio) * cosine\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train / Predict\n",
    "# -------------------------\n",
    "def train_model(model, train_loader, val_loader, y_train, y_val, device,\n",
    "                epochs=120, patience=12, lr=1e-3, weight_decay=1e-4,\n",
    "                use_amp=True, warmup_ratio=0.08):\n",
    "    model.to(device)\n",
    "\n",
    "    pos_ratio = float(np.mean(y_train))\n",
    "    pos_weight = torch.tensor([(1 - pos_ratio) / (pos_ratio + 1e-6)], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    total_steps = epochs * max(1, len(train_loader))\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = make_cosine_warmup_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.05)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.startswith(\"cuda\")))\n",
    "    best_auc, best_state, no_improve = -1.0, None, 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for X_num, X_cat, y in train_loader:\n",
    "            X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(use_amp and device.startswith(\"cuda\"))):\n",
    "                logits = model(X_num, X_cat)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for X_num, X_cat, _ in val_loader:\n",
    "                X_num, X_cat = X_num.to(device), X_cat.to(device)\n",
    "                with torch.cuda.amp.autocast(enabled=(use_amp and device.startswith(\"cuda\"))):\n",
    "                    p = torch.sigmoid(model(X_num, X_cat))\n",
    "                val_preds.append(p.detach().cpu().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds).ravel()\n",
    "        val_auc = roc_auc_score(y_val, val_preds)\n",
    "\n",
    "        if val_auc > best_auc + 1e-5:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_auc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, device, use_amp=True):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for batch in loader:\n",
    "        X_num, X_cat = batch[0].to(device), batch[1].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=(use_amp and device.startswith(\"cuda\"))):\n",
    "            p = torch.sigmoid(model(X_num, X_cat))\n",
    "        preds.append(p.detach().cpu().numpy())\n",
    "    return np.concatenate(preds).ravel()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# One fold runner (shared)\n",
    "# -------------------------\n",
    "def prepare_fold_data(train_df, val_df, test_df):\n",
    "    tr_fe = build_features(train_df)\n",
    "    va_fe = build_features(val_df)\n",
    "    te_fe = build_features(test_df)\n",
    "\n",
    "    te_dict = create_target_encodings(tr_fe, va_fe, te_fe, \"voted_bin\")\n",
    "    all_num = num_features + te_features\n",
    "\n",
    "    X_tr = tr_fe[num_features].copy()\n",
    "    X_va = va_fe[num_features].copy()\n",
    "    X_te = te_fe[num_features].copy()\n",
    "\n",
    "    for te_name in te_features:\n",
    "        X_tr[te_name] = te_dict[\"train\"][te_name]\n",
    "        X_va[te_name] = te_dict[\"val\"][te_name]\n",
    "        X_te[te_name] = te_dict[\"test\"][te_name]\n",
    "\n",
    "    for c in all_num:\n",
    "        med = X_tr[c].median()\n",
    "        if pd.isna(med):\n",
    "            med = 0.0\n",
    "        X_tr[c] = X_tr[c].fillna(med)\n",
    "        X_va[c] = X_va[c].fillna(med)\n",
    "        X_te[c] = X_te[c].fillna(med)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_s = scaler.fit_transform(X_tr.values)\n",
    "    X_va_s = scaler.transform(X_va.values)\n",
    "    X_te_s = scaler.transform(X_te.values)\n",
    "\n",
    "    cat_dims = []\n",
    "    X_cat_tr, X_cat_va, X_cat_te = [], [], []\n",
    "    for col in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        all_vals = list(\n",
    "            set(tr_fe[col].fillna(\"NaN\").astype(str))\n",
    "            | set(va_fe[col].fillna(\"NaN\").astype(str))\n",
    "            | set(te_fe[col].fillna(\"NaN\").astype(str))\n",
    "        )\n",
    "        le.fit(all_vals + [\"UNK\"])\n",
    "        cat_dims.append(len(le.classes_))\n",
    "\n",
    "        X_cat_tr.append(le.transform(tr_fe[col].fillna(\"NaN\").astype(str)))\n",
    "        X_cat_va.append(le.transform(va_fe[col].fillna(\"NaN\").astype(str)))\n",
    "        X_cat_te.append(le.transform(te_fe[col].fillna(\"NaN\").astype(str)))\n",
    "\n",
    "    X_cat_tr = np.stack(X_cat_tr, axis=1)\n",
    "    X_cat_va = np.stack(X_cat_va, axis=1)\n",
    "    X_cat_te = np.stack(X_cat_te, axis=1)\n",
    "\n",
    "    y_tr = tr_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "    y_va = va_fe[\"voted_bin\"].values.astype(np.float32)\n",
    "\n",
    "    return X_tr_s, X_va_s, X_te_s, X_cat_tr, X_cat_va, X_cat_te, y_tr, y_va, cat_dims\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Optuna Objective (FT only)\n",
    "# -------------------------\n",
    "def objective(trial):\n",
    "    set_seed(TUNE_SEED)\n",
    "\n",
    "    # search space\n",
    "    d_token = trial.suggest_categorical(\"d_token\", [64, 96, 128, 160])\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 6)\n",
    "    n_heads = trial.suggest_categorical(\"n_heads\", [4, 8])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.30)\n",
    "    lr = trial.suggest_float(\"lr\", 3e-4, 2e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.03, 0.15)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [512, 1024])\n",
    "\n",
    "    train_clean = clean_data(train_raw)\n",
    "    test_clean  = clean_data(test_raw)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=TUNE_SEED)\n",
    "    tr_idx, va_idx = next(iter(skf.split(train_clean, train_clean[\"voted_bin\"])))  # ‚úÖ 1 foldÎßå\n",
    "\n",
    "    tr_df = train_clean.iloc[tr_idx].reset_index(drop=True).copy()\n",
    "    va_df = train_clean.iloc[va_idx].reset_index(drop=True).copy()\n",
    "    te_df = test_clean.copy()\n",
    "\n",
    "    X_tr_s, X_va_s, _, X_cat_tr, X_cat_va, _, y_tr, y_va, cat_dims = prepare_fold_data(tr_df, va_df, te_df)\n",
    "\n",
    "    train_ds = TabDataset(X_tr_s, X_cat_tr, y_tr)\n",
    "    val_ds   = TabDataset(X_va_s, X_cat_va, y_va)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True)\n",
    "\n",
    "    ft = FTTransformer(\n",
    "        num_features=X_tr_s.shape[1],\n",
    "        cat_dims=cat_dims,\n",
    "        d_token=d_token,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_heads,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "\n",
    "    ft, auc = train_model(\n",
    "        ft, train_loader, val_loader,\n",
    "        y_train=y_tr, y_val=y_va,\n",
    "        device=DEVICE,\n",
    "        epochs=TUNE_MAX_EPOCHS,\n",
    "        patience=TUNE_PATIENCE,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        use_amp=USE_AMP,\n",
    "        warmup_ratio=warmup_ratio\n",
    "    )\n",
    "\n",
    "    # prune\n",
    "    trial.report(auc, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return auc\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Full Train (with best params)\n",
    "# -------------------------\n",
    "def run_full_training(best_params: dict):\n",
    "    all_results = []\n",
    "\n",
    "    for seed in SEEDS_FULL:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üå± FULL TRAIN seed={seed}\")\n",
    "        print(\"=\" * 80)\n",
    "        set_seed(seed)\n",
    "\n",
    "        train_clean = clean_data(train_raw)\n",
    "        test_clean  = clean_data(test_raw)\n",
    "\n",
    "        oof_mlp = np.zeros(len(train_clean), dtype=np.float32)\n",
    "        oof_ft  = np.zeros(len(train_clean), dtype=np.float32)\n",
    "        test_mlp = np.zeros(len(test_clean), dtype=np.float32)\n",
    "        test_ft  = np.zeros(len(test_clean), dtype=np.float32)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS_FULL, shuffle=True, random_state=seed)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(train_clean, train_clean[\"voted_bin\"]), start=1):\n",
    "            print(f\"\\n[Fold {fold}/{N_FOLDS_FULL}]\")\n",
    "\n",
    "            tr_df = train_clean.iloc[tr_idx].reset_index(drop=True).copy()\n",
    "            va_df = train_clean.iloc[va_idx].reset_index(drop=True).copy()\n",
    "            te_df = test_clean.copy()\n",
    "\n",
    "            X_tr_s, X_va_s, X_te_s, X_cat_tr, X_cat_va, X_cat_te, y_tr, y_va, cat_dims = prepare_fold_data(tr_df, va_df, te_df)\n",
    "\n",
    "            train_ds = TabDataset(X_tr_s, X_cat_tr, y_tr)\n",
    "            val_ds   = TabDataset(X_va_s, X_cat_va, y_va)\n",
    "            test_ds  = TabDataset(X_te_s, X_cat_te, None)\n",
    "\n",
    "            batch_size = int(best_params.get(\"batch_size\", 1024))\n",
    "            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "            val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True)\n",
    "            test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True)\n",
    "\n",
    "            # ---- MLP (fixed)\n",
    "            mlp = MLP(\n",
    "                num_features=X_tr_s.shape[1],\n",
    "                cat_dims=cat_dims,\n",
    "                embed_dim=MLP_CFG[\"embed_dim\"],\n",
    "                hidden_dims=MLP_CFG[\"hidden_dims\"],\n",
    "                dropout=MLP_CFG[\"dropout\"]\n",
    "            )\n",
    "            mlp, _ = train_model(\n",
    "                mlp, train_loader, val_loader,\n",
    "                y_train=y_tr, y_val=y_va,\n",
    "                device=DEVICE,\n",
    "                epochs=EPOCHS,\n",
    "                patience=PATIENCE,\n",
    "                lr=MLP_CFG[\"lr\"],\n",
    "                weight_decay=MLP_CFG[\"weight_decay\"],\n",
    "                use_amp=USE_AMP,\n",
    "                warmup_ratio=0.08\n",
    "            )\n",
    "            oof_mlp[va_idx] = predict(mlp, val_loader, DEVICE, use_amp=USE_AMP)\n",
    "            test_mlp += predict(mlp, test_loader, DEVICE, use_amp=USE_AMP) / N_FOLDS_FULL\n",
    "\n",
    "            # ---- FT (best params)\n",
    "            ft = FTTransformer(\n",
    "                num_features=X_tr_s.shape[1],\n",
    "                cat_dims=cat_dims,\n",
    "                d_token=int(best_params[\"d_token\"]),\n",
    "                n_layers=int(best_params[\"n_layers\"]),\n",
    "                n_heads=int(best_params[\"n_heads\"]),\n",
    "                dropout=float(best_params[\"dropout\"]),\n",
    "            )\n",
    "            ft, _ = train_model(\n",
    "                ft, train_loader, val_loader,\n",
    "                y_train=y_tr, y_val=y_va,\n",
    "                device=DEVICE,\n",
    "                epochs=EPOCHS,\n",
    "                patience=PATIENCE,\n",
    "                lr=float(best_params[\"lr\"]),\n",
    "                weight_decay=float(best_params[\"weight_decay\"]),\n",
    "                use_amp=USE_AMP,\n",
    "                warmup_ratio=float(best_params[\"warmup_ratio\"])\n",
    "            )\n",
    "            oof_ft[va_idx] = predict(ft, val_loader, DEVICE, use_amp=USE_AMP)\n",
    "            test_ft += predict(ft, test_loader, DEVICE, use_amp=USE_AMP) / N_FOLDS_FULL\n",
    "\n",
    "            del mlp, ft, train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n",
    "            if DEVICE.startswith(\"cuda\"):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # weight search per seed\n",
    "        y_all = train_clean[\"voted_bin\"].values.astype(np.float32)\n",
    "        best_w, best_auc = 0.5, -1.0\n",
    "        for w in np.arange(0.0, 1.0 + 1e-9, W_STEP):\n",
    "            oof_w = w * oof_mlp + (1 - w) * oof_ft\n",
    "            auc = roc_auc_score(y_all, oof_w)\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_w = float(w)\n",
    "\n",
    "        test_ens = best_w * test_mlp + (1 - best_w) * test_ft\n",
    "        all_results.append(dict(seed=seed, best_w=best_w, best_auc=best_auc,\n",
    "                                oof_mlp=oof_mlp, oof_ft=oof_ft,\n",
    "                                test_mlp=test_mlp, test_ft=test_ft,\n",
    "                                test_ens=test_ens))\n",
    "\n",
    "        print(f\"seed={seed} | best_auc={best_auc:.5f} | best_w={best_w:.2f}\")\n",
    "\n",
    "    # final seed-avg\n",
    "    y_all = train_raw[\"voted_bin\"].values.astype(np.float32)\n",
    "    oof_ens_all = np.mean([r[\"best_w\"] * r[\"oof_mlp\"] + (1 - r[\"best_w\"]) * r[\"oof_ft\"] for r in all_results], axis=0)\n",
    "    final_oof_auc = roc_auc_score(y_all, oof_ens_all)\n",
    "\n",
    "    test_ens_all = np.mean([r[\"test_ens\"] for r in all_results], axis=0)\n",
    "    test_mlp_all = np.mean([r[\"test_mlp\"] for r in all_results], axis=0)\n",
    "    test_ft_all  = np.mean([r[\"test_ft\"] for r in all_results], axis=0)\n",
    "\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(f\"üèÅ FINAL OOF AUC: {final_oof_auc:.5f}\")\n",
    "    print(\"#\" * 80)\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        \"index\": test_raw[\"index\"] if \"index\" in test_raw.columns else np.arange(len(test_raw)),\n",
    "        \"voted\": test_ens_all\n",
    "    })\n",
    "    sub.to_csv(os.path.join(OUT_DIR, \"submission_colab_v7_optuna_ensemble.csv\"), index=False)\n",
    "\n",
    "    sub_mlp = sub.copy()\n",
    "    sub_mlp[\"voted\"] = test_mlp_all\n",
    "    sub_mlp.to_csv(os.path.join(OUT_DIR, \"submission_colab_v7_optuna_mlp.csv\"), index=False)\n",
    "\n",
    "    sub_ft = sub.copy()\n",
    "    sub_ft[\"voted\"] = test_ft_all\n",
    "    sub_ft.to_csv(os.path.join(OUT_DIR, \"submission_colab_v7_optuna_ft.csv\"), index=False)\n",
    "\n",
    "    # save final report\n",
    "    report = {\n",
    "        \"final_oof_auc\": float(final_oof_auc),\n",
    "        \"seeds\": SEEDS_FULL,\n",
    "        \"best_params\": best_params,\n",
    "        \"seed_results\": [{\"seed\": r[\"seed\"], \"best_auc\": float(r[\"best_auc\"]), \"best_w\": float(r[\"best_w\"])} for r in all_results]\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"report_colab_v7_optuna.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    print(\"üíæ Saved to:\", OUT_DIR)\n",
    "    print(\" - submission_colab_v7_optuna_ensemble.csv\")\n",
    "    print(\" - submission_colab_v7_optuna_mlp.csv\")\n",
    "    print(\" - submission_colab_v7_optuna_ft.csv\")\n",
    "    print(\" - report_colab_v7_optuna.json\")\n",
    "\n",
    "    return final_oof_auc\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Entry\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Colab/JupyterÏóêÏÑúÎäî argparseÍ∞Ä Íº¨Ïùº Ïàò ÏûàÏñ¥ÏÑú ÏïàÏ†ÑÌïòÍ≤å Ï≤òÎ¶¨\n",
    "    parser = argparse.ArgumentParser(add_help=False)\n",
    "    parser.add_argument(\"--mode\", type=str, choices=[\"tune\", \"train\"], default=\"train\")  # ‚úÖ default\n",
    "    parser.add_argument(\"--trials\", type=int, default=60)\n",
    "    args, _ = parser.parse_known_args()  # ‚úÖ Ïïå Ïàò ÏóÜÎäî Ïù∏Ïûê Î¨¥Ïãú\n",
    "\n",
    "    print(f\"üñ•Ô∏è Device: {DEVICE}\")\n",
    "    train_raw = pd.read_csv(TRAIN_PATH)\n",
    "    test_raw  = pd.read_csv(TEST_PATH)\n",
    "    train_raw[\"voted_bin\"] = (train_raw[\"voted\"] == 2).astype(int)\n",
    "\n",
    "    best_path = os.path.join(BASE_DIR, \"best_params_ft.json\")\n",
    "\n",
    "    if args.mode == \"tune\":\n",
    "        print(f\"üîé Optuna tuning... trials={args.trials}\")\n",
    "        sampler = optuna.samplers.TPESampler(seed=TUNE_SEED)\n",
    "        pruner = optuna.pruners.MedianPruner(n_startup_trials=10)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n",
    "        study.optimize(objective, n_trials=args.trials)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        with open(best_path, \"w\") as f:\n",
    "            json.dump(best_params, f, indent=2)\n",
    "        print(\"‚úÖ Best params saved:\", best_path)\n",
    "        print(\"Best AUC:\", study.best_value)\n",
    "        print(\"Best params:\", best_params)\n",
    "\n",
    "    elif args.mode == \"train\":\n",
    "        if not os.path.exists(best_path):\n",
    "            raise FileNotFoundError(f\"Run tune first. Missing: {best_path}\")\n",
    "        with open(best_path, \"r\") as f:\n",
    "            best_params = json.load(f)\n",
    "        print(\"‚úÖ Loaded best params:\", best_params)\n",
    "        run_full_training(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}