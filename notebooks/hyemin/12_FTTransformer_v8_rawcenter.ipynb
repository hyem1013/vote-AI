{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12_FTTransformer_v8_rawcenter ÏöîÏïΩ\n",
    "\n",
    "- Î™®Îç∏: FT-Transformer\n",
    "- ÌîºÏ≤ò: Í∏∞Ï°¥ FE + raw center Î≤ÑÏ†Ñ(run_name=ft_rawcenter)\n",
    "- ÌïôÏäµ/ÌèâÍ∞Ä: KFold 5, seed=42\n",
    "- Ï†úÏ∂úÌååÏùº: submission_12_FTTransformer_v8_rawcenter.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d6f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cpu\n",
      "üìä Config: 5-Fold | batch=512 | epochs=60\n",
      "Train: (45532, 79) | Test: (11383, 77)\n",
      "Target positive rate (voted==2): 54.68%\n",
      "\n",
      "====================================================\n",
      "üìÇ Fold 1/5\n",
      "====================================================\n",
      "    Model params: 168,129 | n_num=79 | n_cat=9\n",
      "    Epoch 010 | loss=0.5024 | val_auc=0.77849 | best=0.77917\n",
      "    Epoch 020 | loss=0.4929 | val_auc=0.77724 | best=0.77926\n",
      "    Early stopping at epoch 23\n",
      "  ‚úÖ Fold 1 best AUC: 0.77926\n",
      "\n",
      "====================================================\n",
      "üìÇ Fold 2/5\n",
      "====================================================\n",
      "    Model params: 168,129 | n_num=79 | n_cat=9\n",
      "    Epoch 010 | loss=0.5022 | val_auc=0.76960 | best=0.76971\n",
      "    Epoch 020 | loss=0.4874 | val_auc=0.76697 | best=0.77138\n",
      "    Early stopping at epoch 23\n",
      "  ‚úÖ Fold 2 best AUC: 0.77138\n",
      "\n",
      "====================================================\n",
      "üìÇ Fold 3/5\n",
      "====================================================\n",
      "    Model params: 168,129 | n_num=79 | n_cat=9\n",
      "    Epoch 010 | loss=0.4968 | val_auc=0.75926 | best=0.76046\n",
      "    Early stopping at epoch 15\n",
      "  ‚úÖ Fold 3 best AUC: 0.76046\n",
      "\n",
      "====================================================\n",
      "üìÇ Fold 4/5\n",
      "====================================================\n",
      "    Model params: 168,129 | n_num=79 | n_cat=9\n",
      "    Epoch 010 | loss=0.4985 | val_auc=0.76536 | best=0.76536\n",
      "    Epoch 020 | loss=0.4834 | val_auc=0.76134 | best=0.76536\n",
      "    Early stopping at epoch 20\n",
      "  ‚úÖ Fold 4 best AUC: 0.76536\n",
      "\n",
      "====================================================\n",
      "üìÇ Fold 5/5\n",
      "====================================================\n",
      "    Model params: 168,129 | n_num=79 | n_cat=9\n",
      "    Epoch 010 | loss=0.4990 | val_auc=0.76366 | best=0.76585\n",
      "    Early stopping at epoch 19\n",
      "  ‚úÖ Fold 5 best AUC: 0.76585\n",
      "\n",
      "============================================================\n",
      "üéâ Final Result\n",
      "============================================================\n",
      "üèÜ OOF AUC: 0.76594\n",
      "üìä Fold AUCs: ['0.77926', '0.77138', '0.76046', '0.76536', '0.76585']\n",
      "üìä Mean ¬± Std: 0.76846 ¬± 0.00641\n",
      "\n",
      "üíæ Saved: submission_ft_rawcenter_seed42.csv\n",
      "   pred range: [0.1778, 0.9897] | mean=0.5241\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 60\n",
    "PATIENCE = 10\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# FT-Transformer params\n",
    "D_MODEL = 64\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 3\n",
    "FFN_MULT = 4\n",
    "DROPOUT = 0.15\n",
    "\n",
    "print(f\"üñ•Ô∏è Device: {DEVICE}\")\n",
    "print(f\"üìä Config: {N_FOLDS}-Fold | batch={BATCH_SIZE} | epochs={EPOCHS}\")\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load\n",
    "# ============================================================\n",
    "train_raw = pd.read_csv(\"../../data/raw/train.csv\")\n",
    "test_raw  = pd.read_csv(\"../../data/raw/test_x.csv\")\n",
    "\n",
    "train_raw[\"voted_bin\"] = (train_raw[\"voted\"] == 2).astype(int)\n",
    "y_full = train_raw[\"voted_bin\"].values.astype(np.float32)\n",
    "\n",
    "print(f\"Train: {train_raw.shape} | Test: {test_raw.shape}\")\n",
    "print(f\"Target positive rate (voted==2): {train_raw['voted_bin'].mean():.2%}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cleaning (Ïù¥ÏÉÅÏπò/Î¨¥ÏùëÎãµ Ï≤òÎ¶¨)\n",
    "# ============================================================\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Î¨¥ÏùëÎãµ(0)ÏùÑ NaNÏúºÎ°ú: Ïã§Ï†ú 0 Í∞íÏù¥ ÏïÑÎãàÎùº 'Î¨¥ÏùëÎãµ' ÏùòÎØ∏\n",
    "    zero_to_nan_cols = [\"education\", \"engnat\", \"hand\", \"married\", \"urban\"]\n",
    "    for col in zero_to_nan_cols:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    # familysize Ïù¥ÏÉÅÏπò\n",
    "    if \"familysize\" in df.columns:\n",
    "        df.loc[df[\"familysize\"] == 0, \"familysize\"] = np.nan\n",
    "        df.loc[df[\"familysize\"] > 15, \"familysize\"] = np.nan\n",
    "\n",
    "    # TP: 0ÏùÄ Î¨¥ÏùëÎãµ\n",
    "    tp_cols = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "    for col in tp_cols:\n",
    "        if col in df.columns:\n",
    "            df.loc[df[col] == 0, col] = np.nan\n",
    "\n",
    "    # Q_E ÏùëÎãµÏãúÍ∞Ñ: heavy-tail -> clip\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    for col in qe_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(lower=100, upper=60000)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3) Feature build (ÏõêÎ≥∏ Ï§ëÏã¨ + ÏµúÏÜå ÌååÏÉù)\n",
    "#    - ÏõêÎ≥∏ QA/QE/TP/WR/WF Ïú†ÏßÄ\n",
    "#    - QEÎäî log ÌååÏÉù(ÏõêÎ≥∏ timeÏùÄ Î≤ÑÎ¶¨Í≥† logÎßå ÏÇ¨Ïö©)\n",
    "#    - Í≤∞Ï∏° ÏûêÏ≤¥Í∞Ä Ïã†Ìò∏Ïù∏ Î∏îÎ°ù: missing ratio Ï∂îÍ∞Ä\n",
    "# ============================================================\n",
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---- Categorical base (Î¨∏Ïûê/Î≤îÏ£ºÎ°ú ÎëêÎäî Í≤å ÏûÑÎ≤†Îî©Ïóê Ïú†Î¶¨)\n",
    "    # age_group, gender, race, religionÏùÄ ÏõêÎûò Î≤îÏ£ºÌòï\n",
    "    # education/married/urban/engnat/handÎèÑ Í∞í Ï¢ÖÎ•ò Ï†ÅÏñ¥ÏÑú catÎ°ú Ïì∞Îäî Í≤å Ï¢ÖÏ¢Ö Îçî Ï¢ãÏùå\n",
    "    # (Îã®, ÏàòÏπòÌòïÏúºÎ°úÎèÑ ÏùºÎ∂Ä ÌååÏÉùÏùÑ ÎßåÎì§Í∏¥ Ìï®)\n",
    "    # Ïó¨Í∏∞ÏÑúÎäî catÎ°úÎßå ÏÇ¨Ïö©ÌïòÍ≥†, ÏàòÏπòÌòï ÌååÏÉùÏùÄ Î≥ÑÎèÑ ÏàòÏπò Ïª¨ÎüºÏúºÎ°ú Îë†.\n",
    "\n",
    "    # ---- Numeric block: Q_A (1~5 ordinal) ÏõêÎ≥∏ 20Í∞ú Í∑∏ÎåÄÎ°ú\n",
    "    qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "\n",
    "    # ---- Numeric block: Q_E -> log\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    for col in qe_cols:\n",
    "        df[f\"{col}_log\"] = np.log1p(df[col])\n",
    "    qe_log_cols = [f\"{col}_log\" for col in qe_cols]\n",
    "\n",
    "    # ---- Numeric block: TP 10Í∞ú ÏõêÎ≥∏ (NaN ÌóàÏö©)\n",
    "    tp_cols = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "\n",
    "    # ---- Numeric block: WR/WF ÏõêÎ≥∏\n",
    "    wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "\n",
    "    # ---- Minimal derived (Î™®Îç∏Ïù¥ Ïä§Ïä§Î°ú ÌïôÏäµÌïòÍ∏∞ Ïñ¥Î†µÍ±∞ÎÇò, Í≤∞Ï∏°/ÏÑ±ÏùòÏã†Ìò∏Îäî Í∞ïÏ†ú Ï£ºÏûÖ)\n",
    "    df[\"qa_mean\"] = df[qa_cols].mean(axis=1)\n",
    "    df[\"qa_std\"]  = df[qa_cols].std(axis=1)\n",
    "    df[\"qa_all_same\"] = (df[qa_cols].std(axis=1) == 0).astype(int)\n",
    "\n",
    "    df[\"qe_log_mean\"] = df[qe_log_cols].mean(axis=1)\n",
    "    df[\"qe_log_std\"]  = df[qe_log_cols].std(axis=1)\n",
    "    df[\"qe_fast_ratio\"] = (df[qe_cols] < 500).sum(axis=1) / 20\n",
    "    df[\"qe_slow_ratio\"] = (df[qe_cols] > 10000).sum(axis=1) / 20\n",
    "\n",
    "    df[\"tp_missing_ratio\"] = df[tp_cols].isna().sum(axis=1) / 10\n",
    "\n",
    "    df[\"wr_sum\"] = df[wr_cols].sum(axis=1)\n",
    "    df[\"wf_sum\"] = df[wf_cols].sum(axis=1)\n",
    "    df[\"word_credibility\"] = df[\"wr_sum\"] - df[\"wf_sum\"]\n",
    "\n",
    "    # ÏÑ±ÏùòÏóÜÏùå Ïã†Ìò∏(ÏïÑÏ£º Í∞ïÌï®)\n",
    "    df[\"is_careless\"] = ((df[qe_cols].mean(axis=1) < 500) | (df[\"qa_all_same\"] == 1)).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 4) Column definitions\n",
    "# ============================================================\n",
    "QA_COLS = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "QE_LOG_COLS = [f\"Q{c}E_log\" for c in \"abcdefghijklmnopqrst\"]\n",
    "TP_COLS = [f\"tp{i:02d}\" for i in range(1, 11)]\n",
    "WR_COLS = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "WF_COLS = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "\n",
    "NUM_COLS = (\n",
    "    QA_COLS\n",
    "    + QE_LOG_COLS\n",
    "    + TP_COLS\n",
    "    + WR_COLS\n",
    "    + WF_COLS\n",
    "    + [\n",
    "        \"familysize\",\n",
    "        \"qa_mean\",\"qa_std\",\"qa_all_same\",\n",
    "        \"qe_log_mean\",\"qe_log_std\",\"qe_fast_ratio\",\"qe_slow_ratio\",\n",
    "        \"tp_missing_ratio\",\n",
    "        \"wr_sum\",\"wf_sum\",\"word_credibility\",\n",
    "        \"is_careless\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "CAT_COLS = [\n",
    "    \"age_group\", \"gender\", \"race\", \"religion\",\n",
    "    \"education\", \"married\", \"urban\", \"engnat\", \"hand\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 5) Simple label encoding with UNK/NAN handling (per fold)\n",
    "# ============================================================\n",
    "def fit_cat_maps(train_df: pd.DataFrame, cat_cols):\n",
    "    maps = {}\n",
    "    sizes = []\n",
    "    for c in cat_cols:\n",
    "        s = train_df[c].copy()\n",
    "        s = s.fillna(\"__NAN__\").astype(str)\n",
    "        uniq = s.unique().tolist()\n",
    "        # reserve: 0=__UNK__, 1.. = known\n",
    "        m = {v: i+1 for i, v in enumerate(uniq)}\n",
    "        maps[c] = m\n",
    "        sizes.append(len(m) + 1)  # +1 for __UNK__ at 0\n",
    "    return maps, sizes\n",
    "\n",
    "def transform_cats(df: pd.DataFrame, cat_cols, maps):\n",
    "    arrs = []\n",
    "    for c in cat_cols:\n",
    "        s = df[c].fillna(\"__NAN__\").astype(str)\n",
    "        m = maps[c]\n",
    "        enc = s.map(lambda x: m.get(x, 0)).astype(np.int64).values\n",
    "        arrs.append(enc)\n",
    "    return np.stack(arrs, axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 6) Dataset\n",
    "# ============================================================\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "# ============================================================\n",
    "# 7) FT-Transformer (feature-token Î∞©Ïãù)\n",
    "#   - Í∞Å numeric feature -> (x * W + b) Î°ú d_model ÌÜ†ÌÅ∞\n",
    "#   - categorical -> embedding ÌÜ†ÌÅ∞\n",
    "#   - [CLS] ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä ÌõÑ Transformer encoder\n",
    "# ============================================================\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, n_num, cat_sizes, d_model=64, n_heads=8, n_layers=3, ffn_mult=4, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.n_num = n_num\n",
    "        self.n_cat = len(cat_sizes)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # numeric feature-token parameters\n",
    "        self.num_weight = nn.Parameter(torch.randn(n_num, d_model) * 0.02)\n",
    "        self.num_bias   = nn.Parameter(torch.zeros(n_num, d_model))\n",
    "\n",
    "        # categorical embeddings (each cat feature = one token)\n",
    "        self.cat_embeds = nn.ModuleList([\n",
    "            nn.Embedding(sz, d_model) for sz in cat_sizes\n",
    "        ])\n",
    "\n",
    "        # CLS token\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * ffn_mult,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        for emb in self.cat_embeds:\n",
    "            nn.init.normal_(emb.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # x_num: [B, n_num]\n",
    "        # numeric tokens: [B, n_num, d_model] = x[:,:,None]*W + b\n",
    "        num_tokens = x_num.unsqueeze(-1) * self.num_weight.unsqueeze(0) + self.num_bias.unsqueeze(0)\n",
    "\n",
    "        # categorical tokens: concat along token dimension\n",
    "        cat_tokens = []\n",
    "        for i, emb in enumerate(self.cat_embeds):\n",
    "            cat_tokens.append(emb(x_cat[:, i]))  # [B, d_model]\n",
    "        if len(cat_tokens) > 0:\n",
    "            cat_tokens = torch.stack(cat_tokens, dim=1)  # [B, n_cat, d_model]\n",
    "            tokens = torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "        else:\n",
    "            tokens = num_tokens\n",
    "\n",
    "        # prepend CLS\n",
    "        cls = self.cls.expand(tokens.size(0), -1, -1)  # [B,1,d_model]\n",
    "        tokens = torch.cat([cls, tokens], dim=1)       # [B,1+n_tokens,d_model]\n",
    "\n",
    "        out = self.encoder(tokens)\n",
    "        cls_out = out[:, 0]  # CLS representation\n",
    "        logits = self.head(cls_out)\n",
    "        return logits\n",
    "\n",
    "# ============================================================\n",
    "# 8) Train / Predict\n",
    "# ============================================================\n",
    "def train_one_fold(model, train_loader, val_loader, y_train, y_val, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # pos_weightÎäî \"train fold\" Í∏∞Ï§Ä\n",
    "    pos_ratio = float(np.mean(y_train))\n",
    "    pos_weight = torch.tensor([(1 - pos_ratio) / (pos_ratio + 1e-6)], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "\n",
    "        for Xn, Xc, yb in train_loader:\n",
    "            Xn, Xc, yb = Xn.to(device), Xc.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xn, Xc)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for Xn, Xc, _ in val_loader:\n",
    "                Xn, Xc = Xn.to(device), Xc.to(device)\n",
    "                p = torch.sigmoid(model(Xn, Xc)).detach().cpu().numpy().ravel()\n",
    "                preds.append(p)\n",
    "        preds = np.concatenate(preds)\n",
    "        val_auc = roc_auc_score(y_val, preds)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-5:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"    Epoch {epoch:03d} | loss={np.mean(tr_losses):.4f} | val_auc={val_auc:.5f} | best={best_auc:.5f}\")\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"    Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, best_auc\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 2:\n",
    "                Xn, Xc = batch\n",
    "            else:\n",
    "                Xn, Xc, _ = batch\n",
    "            Xn, Xc = Xn.to(device), Xc.to(device)\n",
    "            p = torch.sigmoid(model(Xn, Xc)).detach().cpu().numpy().ravel()\n",
    "            preds.append(p)\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# ============================================================\n",
    "# 9) Main CV Loop\n",
    "# ============================================================\n",
    "def main(run_name=\"ft_base\"):\n",
    "    set_seed(SEED)\n",
    "\n",
    "    train_clean = clean_data(train_raw)\n",
    "    test_clean  = clean_data(test_raw)\n",
    "\n",
    "    train_fe = build_features(train_clean)\n",
    "    test_fe  = build_features(test_clean)\n",
    "\n",
    "    oof = np.zeros(len(train_fe), dtype=np.float32)\n",
    "    test_pred = np.zeros(len(test_fe), dtype=np.float32)\n",
    "    fold_aucs = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_fe, y_full), 1):\n",
    "        print(\"\\n\" + \"=\"*52)\n",
    "        print(f\"üìÇ Fold {fold}/{N_FOLDS}\")\n",
    "        print(\"=\"*52)\n",
    "\n",
    "        tr_df = train_fe.iloc[tr_idx].copy().reset_index(drop=True)\n",
    "        va_df = train_fe.iloc[va_idx].copy().reset_index(drop=True)\n",
    "        te_df = test_fe.copy().reset_index(drop=True)\n",
    "\n",
    "        y_tr = y_full[tr_idx]\n",
    "        y_va = y_full[va_idx]\n",
    "\n",
    "        # --------- numeric: NaN -> train median, then QuantileTransformer\n",
    "        X_tr_num = tr_df[NUM_COLS].copy()\n",
    "        X_va_num = va_df[NUM_COLS].copy()\n",
    "        X_te_num = te_df[NUM_COLS].copy()\n",
    "\n",
    "        for c in NUM_COLS:\n",
    "            med = X_tr_num[c].median()\n",
    "            if pd.isna(med):\n",
    "                med = 0.0\n",
    "            X_tr_num[c] = X_tr_num[c].fillna(med)\n",
    "            X_va_num[c] = X_va_num[c].fillna(med)\n",
    "            X_te_num[c] = X_te_num[c].fillna(med)\n",
    "\n",
    "        scaler = QuantileTransformer(\n",
    "            n_quantiles=2000,\n",
    "            output_distribution=\"normal\",\n",
    "            random_state=SEED\n",
    "        )\n",
    "        X_tr_num = scaler.fit_transform(X_tr_num.values)\n",
    "        X_va_num = scaler.transform(X_va_num.values)\n",
    "        X_te_num = scaler.transform(X_te_num.values)\n",
    "\n",
    "        # --------- categorical: fit on train fold only (UNK=0)\n",
    "        cat_maps, cat_sizes = fit_cat_maps(tr_df, CAT_COLS)\n",
    "        X_tr_cat = transform_cats(tr_df, CAT_COLS, cat_maps)\n",
    "        X_va_cat = transform_cats(va_df, CAT_COLS, cat_maps)\n",
    "        X_te_cat = transform_cats(te_df, CAT_COLS, cat_maps)\n",
    "\n",
    "        train_ds = TabDataset(X_tr_num, X_tr_cat, y_tr)\n",
    "        val_ds   = TabDataset(X_va_num, X_va_cat, y_va)\n",
    "        test_ds  = TabDataset(X_te_num, X_te_cat)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = FTTransformer(\n",
    "            n_num=X_tr_num.shape[1],\n",
    "            cat_sizes=cat_sizes,\n",
    "            d_model=D_MODEL,\n",
    "            n_heads=N_HEADS,\n",
    "            n_layers=N_LAYERS,\n",
    "            ffn_mult=FFN_MULT,\n",
    "            dropout=DROPOUT\n",
    "        )\n",
    "\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"    Model params: {n_params:,} | n_num={X_tr_num.shape[1]} | n_cat={len(cat_sizes)}\")\n",
    "\n",
    "        model, best_auc = train_one_fold(model, train_loader, val_loader, y_tr, y_va, DEVICE)\n",
    "        fold_aucs.append(best_auc)\n",
    "\n",
    "        oof[va_idx] = predict(model, val_loader, DEVICE)\n",
    "        test_pred += predict(model, test_loader, DEVICE) / N_FOLDS\n",
    "\n",
    "        print(f\"  ‚úÖ Fold {fold} best AUC: {best_auc:.5f}\")\n",
    "\n",
    "    oof_auc = roc_auc_score(y_full, oof)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ Final Result\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üèÜ OOF AUC: {oof_auc:.5f}\")\n",
    "    print(f\"üìä Fold AUCs: {[f'{x:.5f}' for x in fold_aucs]}\")\n",
    "    print(f\"üìä Mean ¬± Std: {np.mean(fold_aucs):.5f} ¬± {np.std(fold_aucs):.5f}\")\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        \"index\": test_raw[\"index\"] if \"index\" in test_raw.columns else np.arange(len(test_raw)),\n",
    "        \"voted\": test_pred\n",
    "    })\n",
    "    out_path = \"submission_12_FTTransformer_v8_rawcenter.csv\"\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"\\nüíæ Saved: {out_path}\")\n",
    "    print(f\"   pred range: [{test_pred.min():.4f}, {test_pred.max():.4f}] | mean={test_pred.mean():.4f}\")\n",
    "\n",
    "    return oof_auc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(run_name=\"ft_rawcenter\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}