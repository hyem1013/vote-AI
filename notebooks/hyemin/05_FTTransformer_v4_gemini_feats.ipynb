{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_FTTransformer_v4_gemini_feats 요약\n",
    "\n",
    "- 모델: FT-Transformer\n",
    "- 피처: v1 베이스 + Gemini 요약(응답시간/문항통계/신뢰도)\n",
    "- 학습/평가: 단일 split\n",
    "- 제출파일: submission_05_FTTransformer_v4_gemini_feats.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 설정값 및 시드 고정\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================\n",
    "# CONFIG\n",
    "# =============================\n",
    "SEED = 42\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50\n",
    "PATIENCE = 6\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"../../data/raw/train.csv\")\n",
    "test_raw  = pd.read_csv(\"../../data/raw/test_x.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 피처 생성 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Feature Engineering\n",
    "# =============================\n",
    "def build_features(df_raw, cfg=None, is_train=True):\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # 0) target\n",
    "    if is_train and \"voted\" in df.columns:\n",
    "        df[\"voted_bin\"] = (df[\"voted\"] == 2).astype(int)\n",
    "\n",
    "    # 1) age_group ordinal\n",
    "    if \"age_group\" in df.columns:\n",
    "        age_map = {\"10s\":1, \"20s\":2, \"30s\":3, \"40s\":4, \"50s\":5, \"60s\":6, \"+70s\":7}\n",
    "        df[\"age_group_ord\"] = df[\"age_group\"].map(age_map).astype(\"float32\")\n",
    "\n",
    "    # 2) education (0 -> NaN -> train mean)\n",
    "    if \"education\" in df.columns:\n",
    "        df[\"education\"] = pd.to_numeric(df[\"education\"], errors=\"coerce\")\n",
    "        df.loc[df[\"education\"] == 0, \"education\"] = np.nan\n",
    "        if cfg is not None:\n",
    "            df[\"education\"] = df[\"education\"].fillna(cfg[\"education_mean\"])\n",
    "        df[\"education\"] = df[\"education\"].astype(\"float32\")\n",
    "\n",
    "    # 3) married_cat\n",
    "    if \"married\" in df.columns:\n",
    "        df[\"married\"] = pd.to_numeric(df[\"married\"], errors=\"coerce\")\n",
    "        df.loc[df[\"married\"] == 0, \"married\"] = np.nan\n",
    "        df[\"married_cat\"] = df[\"married\"].astype(\"string\")\n",
    "\n",
    "    # 4) hand_cat\n",
    "    if \"hand\" in df.columns:\n",
    "        df[\"hand\"] = pd.to_numeric(df[\"hand\"], errors=\"coerce\")\n",
    "        df.loc[df[\"hand\"] == 0, \"hand\"] = np.nan\n",
    "        df[\"hand_cat\"] = df[\"hand\"].astype(\"string\")\n",
    "\n",
    "    # 5) urban_ord\n",
    "    if \"urban\" in df.columns:\n",
    "        df[\"urban\"] = pd.to_numeric(df[\"urban\"], errors=\"coerce\")\n",
    "        df.loc[df[\"urban\"] == 0, \"urban\"] = np.nan\n",
    "        df[\"urban_ord\"] = df[\"urban\"].astype(\"float32\")\n",
    "\n",
    "    # 6) race/religion simple\n",
    "    def simplify_major_other(series, majors):\n",
    "        return series.apply(lambda x: x if x in majors else \"Other\")\n",
    "\n",
    "    if \"race\" in df.columns and cfg is not None:\n",
    "        df[\"race_simple\"] = simplify_major_other(df[\"race\"], cfg[\"race_majors\"]).astype(str)\n",
    "\n",
    "    if \"religion\" in df.columns and cfg is not None:\n",
    "        df[\"religion_simple\"] = simplify_major_other(df[\"religion\"], cfg[\"religion_majors\"]).astype(str)\n",
    "\n",
    "    # -----------------------------\n",
    "    # (BASE) Q_A 요약\n",
    "    # -----------------------------\n",
    "    neg_cols = [\"QbA\",\"QcA\",\"QjA\",\"QmA\",\"QoA\",\"QsA\"]\n",
    "    pos_cols = [\"QkA\",\"QqA\"]\n",
    "    other_cols = [\"QeA\",\"QfA\",\"QhA\",\"QrA\"]\n",
    "\n",
    "    for col in neg_cols + pos_cols + other_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if all(c in df.columns for c in neg_cols):\n",
    "        df[\"neg_att\"] = df[neg_cols].mean(axis=1)\n",
    "\n",
    "    if all(c in df.columns for c in pos_cols):\n",
    "        df[\"pos_att\"] = df[pos_cols].mean(axis=1)\n",
    "\n",
    "    if all(c in df.columns for c in other_cols):\n",
    "        other = df[other_cols]\n",
    "        df[\"neutral_ratio\"] = (other == 3).mean(axis=1).astype(\"float32\")\n",
    "        df[\"confident_ratio\"] = ((other <= 2) | (other >= 4)).mean(axis=1).astype(\"float32\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # (BASE) Big5 diff/strength\n",
    "    # -----------------------------\n",
    "    tp_pairs = {\n",
    "        \"extraversion\": (\"tp01\", \"tp06\"),\n",
    "        \"agreeableness\": (\"tp07\", \"tp02\"),\n",
    "        \"conscientiousness\": (\"tp03\", \"tp08\"),\n",
    "        \"neuroticism\": (\"tp04\", \"tp09\"),\n",
    "        \"openness\": (\"tp05\", \"tp10\"),\n",
    "    }\n",
    "    for trait, (a, b) in tp_pairs.items():\n",
    "        if a in df.columns and b in df.columns:\n",
    "            df[a] = pd.to_numeric(df[a], errors=\"coerce\")\n",
    "            df[b] = pd.to_numeric(df[b], errors=\"coerce\")\n",
    "            df[f\"{trait}_diff\"] = (df[a] - df[b]).astype(\"float32\")\n",
    "            df[f\"{trait}_strength\"] = df[f\"{trait}_diff\"].abs().astype(\"float32\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # (BASE) word sums\n",
    "    # -----------------------------\n",
    "    wr_cols = [f\"wr_{i:02d}\" for i in range(1, 14)]\n",
    "    wf_cols = [f\"wf_{i:02d}\" for i in range(1, 4)]\n",
    "\n",
    "    if all(c in df.columns for c in wr_cols):\n",
    "        df[\"wr_sum\"] = df[wr_cols].sum(axis=1).astype(\"float32\")\n",
    "    if all(c in df.columns for c in wf_cols):\n",
    "        df[\"wf_sum\"] = df[wf_cols].sum(axis=1).astype(\"float32\")\n",
    "\n",
    "    if \"wr_sum\" in df.columns and \"wf_sum\" in df.columns:\n",
    "        df[\"word_credibility\"] = (df[\"wr_sum\"] - df[\"wf_sum\"]).astype(\"float32\")\n",
    "        # Gemini 버전(패널티 강화)도 추가\n",
    "        df[\"reliability_score\"] = (df[\"wr_sum\"] - 2.0 * df[\"wf_sum\"]).astype(\"float32\")\n",
    "\n",
    "    if \"word_credibility\" in df.columns:\n",
    "        df[\"cred_bin\"] = pd.cut(\n",
    "            df[\"word_credibility\"],\n",
    "            bins=[-3, 1, 6, 13],\n",
    "            labels=[\"Low\", \"Mid\", \"High\"]\n",
    "        )\n",
    "\n",
    "    # =============================\n",
    "    # (ADD: Gemini summary only)\n",
    "    # =============================\n",
    "\n",
    "    # (1) total_time_log: Q?E 전체 로그합\n",
    "    qe_cols = [f\"Q{c}E\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    if all(c in df.columns for c in qe_cols):\n",
    "        for c in qe_cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df[\"total_time_log\"] = np.log1p(df[qe_cols]).sum(axis=1).astype(\"float32\")\n",
    "\n",
    "    # (2) mach_score / mach_std (역채점 포함)\n",
    "    qa_cols = [f\"Q{c}A\" for c in \"abcdefghijklmnopqrst\"]\n",
    "    reverse_cols = [\"QaA\",\"QdA\",\"QgA\",\"QiA\",\"QlA\",\"QnA\",\"QpA\",\"QrA\",\"QtA\"]\n",
    "    if all(c in df.columns for c in qa_cols):\n",
    "        for c in qa_cols:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        for c in reverse_cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = 6 - df[c]  # 1~5 가정\n",
    "        df[\"mach_score\"] = df[qa_cols].mean(axis=1).astype(\"float32\")\n",
    "        df[\"mach_std\"] = df[qa_cols].std(axis=1).astype(\"float32\")\n",
    "\n",
    "    # (3) age_edu interaction (categorical)\n",
    "    if \"age_group\" in df.columns and \"education\" in df.columns:\n",
    "        df[\"age_edu\"] = df[\"age_group\"].astype(\"string\") + \"_\" + df[\"education\"].astype(\"string\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 피처 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Feature columns\n",
    "# =============================\n",
    "target = \"voted_bin\"\n",
    "\n",
    "# (기본 + Gemini 요약만 추가)\n",
    "feature_cols = [\n",
    "    # base\n",
    "    \"neg_att\", \"pos_att\", \"neutral_ratio\", \"confident_ratio\",\n",
    "    \"extraversion_diff\", \"extraversion_strength\",\n",
    "    \"agreeableness_diff\", \"agreeableness_strength\",\n",
    "    \"conscientiousness_diff\", \"conscientiousness_strength\",\n",
    "    \"neuroticism_diff\", \"neuroticism_strength\",\n",
    "    \"openness_diff\", \"openness_strength\",\n",
    "    \"wr_sum\", \"wf_sum\", \"word_credibility\", \"cred_bin\",\n",
    "    \"age_group_ord\", \"education\", \"urban_ord\",\n",
    "    \"hand_cat\", \"married_cat\",\n",
    "    \"race_simple\", \"religion_simple\",\n",
    "\n",
    "    # add (gemini summary only)\n",
    "    \"total_time_log\",      # 전체 시간 로그합\n",
    "    \"mach_score\", \"mach_std\",\n",
    "    \"reliability_score\",\n",
    "    \"age_edu\",             # interaction (cat)\n",
    "]\n",
    "\n",
    "cat_cols = [\"race_simple\", \"religion_simple\", \"cred_bin\", \"hand_cat\", \"married_cat\", \"age_edu\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 전처리 함수 (인코딩/스케일링)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _normalize_cat(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.fillna(\"__MISSING__\")\n",
    "    s = s.replace({\"<NA>\": \"__MISSING__\", \"nan\": \"__MISSING__\", \"NaN\": \"__MISSING__\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "def _fit_label_encoders(df_train, cat_cols):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        s = _normalize_cat(df_train[col])\n",
    "        if \"__MISSING__\" not in s.unique():\n",
    "            s = pd.concat([s, pd.Series([\"__MISSING__\"])], ignore_index=True)\n",
    "        le.fit(s.astype(str))\n",
    "        encoders[col] = le\n",
    "    return encoders\n",
    "\n",
    "\n",
    "def _transform_with_encoder(series, le: LabelEncoder):\n",
    "    s = _normalize_cat(series)\n",
    "    classes = set(le.classes_)\n",
    "    s = s.apply(lambda x: x if x in classes else \"__MISSING__\")\n",
    "    return le.transform(s.astype(str))\n",
    "\n",
    "\n",
    "def preprocess_split_fit_transform(df_train, df_val, df_test, feature_cols, cat_cols, target):\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "\n",
    "    encoders = _fit_label_encoders(df_train, cat_cols)\n",
    "\n",
    "    X_cat_train = np.stack([_transform_with_encoder(df_train[c], encoders[c]) for c in cat_cols], axis=1)\n",
    "    X_cat_val   = np.stack([_transform_with_encoder(df_val[c], encoders[c])   for c in cat_cols], axis=1)\n",
    "    X_cat_test  = np.stack([_transform_with_encoder(df_test[c], encoders[c])  for c in cat_cols], axis=1)\n",
    "\n",
    "    num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "    train_num = df_train[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    val_num   = df_val[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test_num  = df_test[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    train_means = train_num.mean()\n",
    "    train_num = train_num.fillna(train_means)\n",
    "    val_num   = val_num.fillna(train_means)\n",
    "    test_num  = test_num.fillna(train_means)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_num_train = scaler.fit_transform(train_num.values)\n",
    "    X_num_val   = scaler.transform(val_num.values)\n",
    "    X_num_test  = scaler.transform(test_num.values)\n",
    "\n",
    "    return {\n",
    "        \"X_num_train\": X_num_train,\n",
    "        \"X_num_val\": X_num_val,\n",
    "        \"X_num_test\": X_num_test,\n",
    "        \"X_cat_train\": X_cat_train,\n",
    "        \"X_cat_val\": X_cat_val,\n",
    "        \"X_cat_test\": X_cat_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"encoders\": encoders,\n",
    "        \"scaler\": scaler,\n",
    "        \"num_cols\": num_cols,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Val 분리 및 피처 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split\n",
    "raw_y = (train_raw[\"voted\"] == 2).astype(int)\n",
    "train_raw_split, val_raw_split = train_test_split(\n",
    "    train_raw, test_size=0.2, random_state=SEED, stratify=raw_y\n",
    ")\n",
    "\n",
    "cfg = {}\n",
    "edu = pd.to_numeric(train_raw_split[\"education\"], errors=\"coerce\").replace(0, np.nan)\n",
    "cfg[\"education_mean\"] = float(edu.mean())\n",
    "cfg[\"race_majors\"] = set(train_raw_split[\"race\"].value_counts(dropna=True).head(5).index)\n",
    "cfg[\"religion_majors\"] = set(train_raw_split[\"religion\"].value_counts(dropna=True).head(5).index)\n",
    "\n",
    "train_feat = build_features(train_raw_split, cfg=cfg, is_train=True)\n",
    "val_feat   = build_features(val_raw_split,   cfg=cfg, is_train=True)\n",
    "test_feat  = build_features(test_raw,        cfg=cfg, is_train=False)\n",
    "\n",
    "prep = preprocess_split_fit_transform(train_feat, val_feat, test_feat, feature_cols, cat_cols, target)\n",
    "\n",
    "X_num_train = prep[\"X_num_train\"]\n",
    "X_num_val   = prep[\"X_num_val\"]\n",
    "X_num_test  = prep[\"X_num_test\"]\n",
    "X_cat_train = prep[\"X_cat_train\"]\n",
    "X_cat_val   = prep[\"X_cat_val\"]\n",
    "X_cat_test  = prep[\"X_cat_test\"]\n",
    "y_train     = prep[\"y_train\"]\n",
    "y_val       = prep[\"y_val\"]\n",
    "num_cols    = prep[\"num_cols\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset & 모델 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y=None):\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_num[idx], self.X_cat[idx]\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NumericalEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features, d_token))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        return x * self.weight + self.bias\n",
    "\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, num_features, d_token=64, n_layers=2, n_heads=4, dropout=0.2, attn_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.cat_embeds = nn.ModuleList([nn.Embedding(dim, d_token) for dim in cat_dims])\n",
    "        self.num_embed = NumericalEmbedding(num_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token, nhead=n_heads, dim_feedforward=d_token * 4,\n",
    "            dropout=attn_dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_token),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_token, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)]\n",
    "        cat_tokens = torch.stack(cat_tokens, dim=1) if len(cat_tokens) > 0 else None\n",
    "\n",
    "        num_tokens = self.num_embed(x_num)\n",
    "        tokens = num_tokens if cat_tokens is None else torch.cat([cat_tokens, num_tokens], dim=1)\n",
    "\n",
    "        cls = self.cls_token.expand(tokens.size(0), -1, -1)\n",
    "        tokens = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "        x = self.transformer(tokens)\n",
    "        return self.head(x[:, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 학습/평가 루프\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_model(model, train_loader, val_loader, y_val, device, epochs=EPOCHS, patience=PATIENCE):\n",
    "    model.to(device)\n",
    "\n",
    "    pos_weight = (len(y_train) - y_train.sum()) / (y_train.sum() + 1e-6)\n",
    "    pos_weight_t = torch.tensor([pos_weight], dtype=torch.float32, device=device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    patience_ctr = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb_num, xb_cat, yb in train_loader:\n",
    "            xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb_num, xb_cat)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb_num, xb_cat, _ in val_loader:\n",
    "                xb_num, xb_cat = xb_num.to(device), xb_cat.to(device)\n",
    "                probs = torch.sigmoid(model(xb_num, xb_cat)).cpu().numpy().ravel()\n",
    "                val_probs.append(probs)\n",
    "        val_probs = np.concatenate(val_probs)\n",
    "        val_auc = roc_auc_score(y_val, val_probs)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-4:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1} | val AUC {val_auc:.4f} (best {best_auc:.4f})\")\n",
    "\n",
    "        if patience_ctr >= patience:\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_auc\n",
    "\n",
    "\n",
    "def predict_proba(model, loader, device):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat in loader:\n",
    "            xb_num, xb_cat = xb_num.to(device), xb_cat.to(device)\n",
    "            p = torch.sigmoid(model(xb_num, xb_cat)).cpu().numpy().ravel()\n",
    "            probs.append(p)\n",
    "    return np.concatenate(probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_dims = [len(prep[\"encoders\"][c].classes_) for c in cat_cols]\n",
    "model = FTTransformer(cat_dims=cat_dims, num_features=len(num_cols), d_token=64, n_layers=2, n_heads=4, dropout=0.2, attn_dropout=0.2)\n",
    "\n",
    "train_ds = TabDataset(X_num_train, X_cat_train, y_train)\n",
    "val_ds   = TabDataset(X_num_val,   X_cat_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model, best_auc = train_one_model(model, train_loader, val_loader, y_val, DEVICE)\n",
    "print(f\"Best Validation ROC-AUC: {best_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 테스트 예측 및 제출\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_ds = TabDataset(X_num_test, X_cat_test, y=None)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_probs = predict_proba(model, test_loader, DEVICE)\n",
    "\n",
    "sub = pd.read_csv(\"../../data/raw/sample_submission.csv\")\n",
    "sub[\"voted\"] = test_probs\n",
    "sub.to_csv(\"submission_05_FTTransformer_v4_gemini_feats.csv\", index=False)\n",
    "print(\"saved submission_05_FTTransformer_v4_gemini_feats.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voteai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}